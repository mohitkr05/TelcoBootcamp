{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Telecom Boot Camp Notes - Updated for Cohort 1 - (Nov-2023 to Feb-2024) Topics to be covered in this Boot Camp Introduction to Telecom and Cloud Concepts Linux & Git Fundamentals Virtualization in Telecom Introduction to Containers Telecom Cloud Services Workflow S.No Topic Chapter Excercise 1 Introduction to Telecom and Cloud Concepts Introduction to Communications and Telecommunication Preparation 2 Introduction to Telecom and Cloud Concepts Cloud Computing and Telco Cloud 3 Linux & Git Fundamentals Linux Fundamentals Linux and Shell scripting 4 Linux & Git Fundamentals Linux Networking - 5 Linux & Git Fundamentals Git Basics Git Hands on Excercises 6 Linux & Git Fundamentals Shell Scripting Linux and Shell scripting 7 Virtualization in Telecom Virtualization in Telecom Virtual Machine HandsOn 9 Introduction to Containers 5 Cloud Computing and Telco Cloud 10 Introduction to Containers 5 5 11 Introduction to Containers 5 5","title":"Telecom Boot Camp"},{"location":"#telecom-boot-camp","text":"Notes - Updated for Cohort 1 - (Nov-2023 to Feb-2024)","title":"Telecom Boot Camp"},{"location":"#topics-to-be-covered-in-this-boot-camp","text":"Introduction to Telecom and Cloud Concepts Linux & Git Fundamentals Virtualization in Telecom Introduction to Containers Telecom Cloud Services","title":"Topics to be covered in this Boot Camp"},{"location":"#workflow","text":"S.No Topic Chapter Excercise 1 Introduction to Telecom and Cloud Concepts Introduction to Communications and Telecommunication Preparation 2 Introduction to Telecom and Cloud Concepts Cloud Computing and Telco Cloud 3 Linux & Git Fundamentals Linux Fundamentals Linux and Shell scripting 4 Linux & Git Fundamentals Linux Networking - 5 Linux & Git Fundamentals Git Basics Git Hands on Excercises 6 Linux & Git Fundamentals Shell Scripting Linux and Shell scripting 7 Virtualization in Telecom Virtualization in Telecom Virtual Machine HandsOn 9 Introduction to Containers 5 Cloud Computing and Telco Cloud 10 Introduction to Containers 5 5 11 Introduction to Containers 5 5","title":"Workflow"},{"location":"excercises/01-preparation/","text":"System preparation Suggest method is to use Virtual Machine with VirtualBox on your local PC, if you have resource constraints you can use WSL as well. Software to be installed VirtualBox : VirtualBox is a powerful open-source virtualization tool that allows you to run multiple operating systems on a single machine. Ideal for development, testing, and experimentation. Python 3 : Python is a versatile and easy-to-learn programming language. Python 3 is the latest version, offering improvements in performance and syntax over its predecessors. Notepad++ : Notepad++ is a feature-rich, free source code editor and Notepad replacement that supports various programming languages. It offers a clean and efficient interface. Kubernetes CLI : The Kubernetes Command Line Interface (kubectl) allows you to interact with Kubernetes clusters. Manage applications, deploy containers, and troubleshoot your cluster from the command line. Kubernetes Helm : Helm is a package manager for Kubernetes applications. It simplifies the deployment and management of complex applications on Kubernetes clusters Wireshark : Wireshark is a powerful network protocol analyzer. It lets you capture and examine the data traveling back and forth on your network, helping with troubleshooting and analysis. PuTTY : PuTTY is a lightweight and reliable terminal emulator for Windows. It supports various network protocols, including SSH, Telnet, and more. Git : Git is a distributed version control system used for tracking changes in source code during software development. It facilitates collaboration and code management. Visual Studio Code (VSCode) : VSCode is a free, open-source code editor with excellent support for various programming languages. It features a wide range of extensions for customization and additional functionality. Docker Desktop : Docker Desktop simplifies the deployment and management of containerized applications. It provides an easy-to-use interface for building, testing, and running Docker containers. AWS Command Line Interface (AWS CLI) : AWS CLI is a unified tool to manage AWS services directly from the command line. It simplifies script automation and interaction with various AWS resources. Installing software on Windows In this chapter, we guide you through the essential steps of installing the required software for your Windows system. The list of softwares to be installed are as follows. You can either directly download from the exe files that are provided or you can use chocolatey to download and install it. To use chocolatey To install Chocolatey, simply open PowerShell and run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) Chocolatey Script # Install Chocolatey packages for DevOps tools choco install virtualbox -y choco install git -y Going forward we are going to install following softwares as well. choco install python3 -y choco install notepadplusplus -y choco install kubernetes-cli -y choco install kubernetes-helm -y choco install wireshark -y choco install putty -y choco install vscode -y choco install docker-desktop -y choco install awscli -y choco install terraform -y choco install packer -y choco install jenkins -y Linux VM Configuration We are going to use the Virtual Machine as our NMS (Network Management Server) Configure SSH port forwarding Add yourself into the sudoers file Login as root with - su - root Add yourself to the sudoers file - echo \"<username> ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers Exit the terminal and test it using - sudo whoami Install OpenSSH Perform the apt-get update using - sudo apt update Install OpenSSH - sudo apt install openssh-server Configure OpenSSH for Password login Start the OpenSSH service - sudo systemctl start ssh Enable the OpenSSH service - sudo systemctl enable ssh Check the status of the OpenSSH service - sudo systemctl status ssh Enable port 22 in UFW - sudo ufw allow 22 Configure the Port forwarding the VM Port forwarding from local port to the server ports Connect using the SSH terminal Login with - ssh <username>@<hostname> System Configuration Set the system name - sudo hostnamectl set-hostname nms Configure the hostnames - sudo vi /etc/hosts Installing Git Update the system - sudo apt-get update Install Git - sudo apt-get install git Configure Git Configure Git git config --global user.name \"Your Name\" git config --global user.email \"Your Email\" Download the Repository Fork the repository https://github.com/mohitkr05/TelcoBootcamp Clone the Repository git clone https://github.com/your-username/TelcoBootcamp.git","title":"System preparation"},{"location":"excercises/01-preparation/#system-preparation","text":"Suggest method is to use Virtual Machine with VirtualBox on your local PC, if you have resource constraints you can use WSL as well.","title":"System preparation"},{"location":"excercises/01-preparation/#software-to-be-installed","text":"VirtualBox : VirtualBox is a powerful open-source virtualization tool that allows you to run multiple operating systems on a single machine. Ideal for development, testing, and experimentation. Python 3 : Python is a versatile and easy-to-learn programming language. Python 3 is the latest version, offering improvements in performance and syntax over its predecessors. Notepad++ : Notepad++ is a feature-rich, free source code editor and Notepad replacement that supports various programming languages. It offers a clean and efficient interface. Kubernetes CLI : The Kubernetes Command Line Interface (kubectl) allows you to interact with Kubernetes clusters. Manage applications, deploy containers, and troubleshoot your cluster from the command line. Kubernetes Helm : Helm is a package manager for Kubernetes applications. It simplifies the deployment and management of complex applications on Kubernetes clusters Wireshark : Wireshark is a powerful network protocol analyzer. It lets you capture and examine the data traveling back and forth on your network, helping with troubleshooting and analysis. PuTTY : PuTTY is a lightweight and reliable terminal emulator for Windows. It supports various network protocols, including SSH, Telnet, and more. Git : Git is a distributed version control system used for tracking changes in source code during software development. It facilitates collaboration and code management. Visual Studio Code (VSCode) : VSCode is a free, open-source code editor with excellent support for various programming languages. It features a wide range of extensions for customization and additional functionality. Docker Desktop : Docker Desktop simplifies the deployment and management of containerized applications. It provides an easy-to-use interface for building, testing, and running Docker containers. AWS Command Line Interface (AWS CLI) : AWS CLI is a unified tool to manage AWS services directly from the command line. It simplifies script automation and interaction with various AWS resources.","title":"Software to be installed"},{"location":"excercises/01-preparation/#installing-software-on-windows","text":"In this chapter, we guide you through the essential steps of installing the required software for your Windows system. The list of softwares to be installed are as follows. You can either directly download from the exe files that are provided or you can use chocolatey to download and install it.","title":"Installing software on Windows"},{"location":"excercises/01-preparation/#to-use-chocolatey","text":"To install Chocolatey, simply open PowerShell and run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))","title":"To use chocolatey"},{"location":"excercises/01-preparation/#chocolatey-script","text":"# Install Chocolatey packages for DevOps tools choco install virtualbox -y choco install git -y Going forward we are going to install following softwares as well. choco install python3 -y choco install notepadplusplus -y choco install kubernetes-cli -y choco install kubernetes-helm -y choco install wireshark -y choco install putty -y choco install vscode -y choco install docker-desktop -y choco install awscli -y choco install terraform -y choco install packer -y choco install jenkins -y","title":"Chocolatey Script"},{"location":"excercises/01-preparation/#linux-vm-configuration","text":"We are going to use the Virtual Machine as our NMS (Network Management Server)","title":"Linux VM Configuration"},{"location":"excercises/01-preparation/#configure-ssh-port-forwarding","text":"Add yourself into the sudoers file Login as root with - su - root Add yourself to the sudoers file - echo \"<username> ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers Exit the terminal and test it using - sudo whoami Install OpenSSH Perform the apt-get update using - sudo apt update Install OpenSSH - sudo apt install openssh-server Configure OpenSSH for Password login Start the OpenSSH service - sudo systemctl start ssh Enable the OpenSSH service - sudo systemctl enable ssh Check the status of the OpenSSH service - sudo systemctl status ssh Enable port 22 in UFW - sudo ufw allow 22 Configure the Port forwarding the VM Port forwarding from local port to the server ports Connect using the SSH terminal Login with - ssh <username>@<hostname>","title":"Configure SSH port forwarding"},{"location":"excercises/01-preparation/#system-configuration","text":"Set the system name - sudo hostnamectl set-hostname nms Configure the hostnames - sudo vi /etc/hosts","title":"System Configuration"},{"location":"excercises/01-preparation/#installing-git","text":"Update the system - sudo apt-get update Install Git - sudo apt-get install git","title":"Installing Git"},{"location":"excercises/01-preparation/#configure-git","text":"Configure Git git config --global user.name \"Your Name\" git config --global user.email \"Your Email\"","title":"Configure Git"},{"location":"excercises/01-preparation/#download-the-repository","text":"Fork the repository https://github.com/mohitkr05/TelcoBootcamp Clone the Repository git clone https://github.com/your-username/TelcoBootcamp.git","title":"Download the Repository"},{"location":"excercises/02-linux-shellscripting/","text":"Linux and Shell scripting Linux excercise Command Line excercise The objective of this excercise is to understand how command line works in Linux Check which shell you are using. Repeat the above command with a short cut key. Create a directory hello and create a file hello.txt in it. Edit the file hello.txt and write Hello World in it. Manipulate the file and perform the folloing Open the file - cat hello.txt Count the lines in the file - wc -l hello.txt Count the first 5 lines in the file - head -n 5 hello.txt Count the last 5 lines in the file - tail -n 5 hello.txt Use the input and output redirection in two of the commands. Use a pipe operator with two commands. Print out the root directory structure in a file with a single line command. Instal wireshark on the system. Remove the package wireshark from the system. Linux as a Multi-user Environment The objective of this excercise is to understand how multiuser environment works in Linux You are a system administrator. Create the following permission scopes Create 3 Groups - sysadmins , netengineers and users Create 4 Users - john , paul , ajay , george and ringo Add users to groups - john in sysadmins paul & ajay in netengineers george & ringo in users Add the user john to sudoers list. Switch user with each of the above user and validate the following Type of shell home directory Create the following directories in /opt/project . Use only a single line command to do this. /opt/project/sysadmin /opt/project/netengineers /opt/project/users Change the ownership of both directories to the following groups owner sysadmin - /opt/project/sysadmin owner netengineers - /opt/project/netengineers owner users - /opt/project/users Validate that both ajay and paul are able to access the documents in the directory /opt/project/netengineers & not able to access any documents in /opt/project/sysadmin . Validate that george and ringo are able to access the documents in the directory /opt/project/users & not able to access any documents in /opt/project/sysadmin . Create a new file in /opt/project/netengineers with the user ajay . Switch to paul - Can you write to this file? How can you do this? Shell scripting hands on","title":"Linux and Shell scripting"},{"location":"excercises/02-linux-shellscripting/#linux-and-shell-scripting","text":"","title":"Linux and Shell scripting"},{"location":"excercises/02-linux-shellscripting/#linux-excercise","text":"","title":"Linux excercise"},{"location":"excercises/02-linux-shellscripting/#command-line-excercise","text":"The objective of this excercise is to understand how command line works in Linux Check which shell you are using. Repeat the above command with a short cut key. Create a directory hello and create a file hello.txt in it. Edit the file hello.txt and write Hello World in it. Manipulate the file and perform the folloing Open the file - cat hello.txt Count the lines in the file - wc -l hello.txt Count the first 5 lines in the file - head -n 5 hello.txt Count the last 5 lines in the file - tail -n 5 hello.txt Use the input and output redirection in two of the commands. Use a pipe operator with two commands. Print out the root directory structure in a file with a single line command. Instal wireshark on the system. Remove the package wireshark from the system.","title":"Command Line excercise"},{"location":"excercises/02-linux-shellscripting/#linux-as-a-multi-user-environment","text":"The objective of this excercise is to understand how multiuser environment works in Linux You are a system administrator. Create the following permission scopes Create 3 Groups - sysadmins , netengineers and users Create 4 Users - john , paul , ajay , george and ringo Add users to groups - john in sysadmins paul & ajay in netengineers george & ringo in users Add the user john to sudoers list. Switch user with each of the above user and validate the following Type of shell home directory Create the following directories in /opt/project . Use only a single line command to do this. /opt/project/sysadmin /opt/project/netengineers /opt/project/users Change the ownership of both directories to the following groups owner sysadmin - /opt/project/sysadmin owner netengineers - /opt/project/netengineers owner users - /opt/project/users Validate that both ajay and paul are able to access the documents in the directory /opt/project/netengineers & not able to access any documents in /opt/project/sysadmin . Validate that george and ringo are able to access the documents in the directory /opt/project/users & not able to access any documents in /opt/project/sysadmin . Create a new file in /opt/project/netengineers with the user ajay . Switch to paul - Can you write to this file? How can you do this?","title":"Linux as a Multi-user Environment"},{"location":"excercises/02-linux-shellscripting/#shell-scripting-hands-on","text":"","title":"Shell scripting hands on"},{"location":"excercises/03-git-handson/","text":"Git Hands on Excercises [ ] Create a new GitHub profile [ ] Configure git on your system [ ] Create a new repository in GitHub [ ] Clone the repository [ ] Change some files [ ] Push the code onto the remote repository [ ] Pull the code from the remote repository [ ] Create a branch and switch to it [ ] Fork the code from TelcoBootcamp [ ] Add a branch [ ] Submit a PR Solution Configure Git: bash git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\" Initialize a Git Repository bash git init Clone a Repository: bash git clone <repository_url> Creates a copy of a remote repository on your local machine. Working with Changes Check Status: bash git status Shows the status of changes as untracked, modified, or staged. Stage Changes: bash git add <file(s)> Adds changes to the staging area in preparation for a commit. Commit Changes: bash git commit -m \"Commit message\" Records staged changes with a descriptive commit message. Branching and Merging Create a Branch: bash git branch <branch_name> Creates a new branch. Switch Branch: bash git checkout <branch_name> Switches to the specified branch. Merge Branch: bash git merge <branch_name> Combines changes from the specified branch into the current branch. Working with Remotes Add a Remote Repository: bash git remote add <remote_name> <remote_url> Adds a remote repository. Push Changes to a Remote Repository: bash git push <remote_name> <branch_name> Pushes committed changes to a remote repository. Pull Changes from a Remote Repository: bash git pull <remote_name> <branch_name> Fetches and merges changes from a remote repository. Checking History View Commit History: bash git log Displays a log of commits. Show Changes in a Commit: bash git show <commit_hash> Displays the changes introduced by a specific commit. Downloading and installing Git : If you don't already have Git installed, you can download Git at www.git-scm.com. > If you need additional assistance installing Git, you can find more information in the ProGit chapter on installing Git. Now is a good time to create a shortcut to the command-line application you will want to use with Git: If you are working on Windows, It is highly recommended to use Git Bash which is installed with the Git package, so that you can follow along with the facilitator who will be using Bash If you are working on macOS or another Unix-like system, you can use the built-in Terminal application Clone the following repository Open your chosen shell, and type: git clone https://github.com/mohitkr05/TelcoBootcamp.git If the clone is successful you'll see: $ git clone https://github.com/mohitkr05/TelcoBootcamp.git Cloning into 'TelcoBootcamp'... remote: Counting objects: 6, done. remote: Compressing objects: 100% (2/2), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (6/6), done.","title":"Git Hands on"},{"location":"excercises/03-git-handson/#git-hands-on","text":"","title":"Git Hands on"},{"location":"excercises/03-git-handson/#excercises","text":"[ ] Create a new GitHub profile [ ] Configure git on your system [ ] Create a new repository in GitHub [ ] Clone the repository [ ] Change some files [ ] Push the code onto the remote repository [ ] Pull the code from the remote repository [ ] Create a branch and switch to it [ ] Fork the code from TelcoBootcamp [ ] Add a branch [ ] Submit a PR","title":"Excercises"},{"location":"excercises/03-git-handson/#solution","text":"Configure Git: bash git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\" Initialize a Git Repository bash git init Clone a Repository: bash git clone <repository_url> Creates a copy of a remote repository on your local machine.","title":"Solution"},{"location":"excercises/03-git-handson/#working-with-changes","text":"Check Status: bash git status Shows the status of changes as untracked, modified, or staged. Stage Changes: bash git add <file(s)> Adds changes to the staging area in preparation for a commit. Commit Changes: bash git commit -m \"Commit message\" Records staged changes with a descriptive commit message.","title":"Working with Changes"},{"location":"excercises/03-git-handson/#branching-and-merging","text":"Create a Branch: bash git branch <branch_name> Creates a new branch. Switch Branch: bash git checkout <branch_name> Switches to the specified branch. Merge Branch: bash git merge <branch_name> Combines changes from the specified branch into the current branch.","title":"Branching and Merging"},{"location":"excercises/03-git-handson/#working-with-remotes","text":"Add a Remote Repository: bash git remote add <remote_name> <remote_url> Adds a remote repository. Push Changes to a Remote Repository: bash git push <remote_name> <branch_name> Pushes committed changes to a remote repository. Pull Changes from a Remote Repository: bash git pull <remote_name> <branch_name> Fetches and merges changes from a remote repository.","title":"Working with Remotes"},{"location":"excercises/03-git-handson/#checking-history","text":"View Commit History: bash git log Displays a log of commits. Show Changes in a Commit: bash git show <commit_hash> Displays the changes introduced by a specific commit. Downloading and installing Git : If you don't already have Git installed, you can download Git at www.git-scm.com. > If you need additional assistance installing Git, you can find more information in the ProGit chapter on installing Git. Now is a good time to create a shortcut to the command-line application you will want to use with Git: If you are working on Windows, It is highly recommended to use Git Bash which is installed with the Git package, so that you can follow along with the facilitator who will be using Bash If you are working on macOS or another Unix-like system, you can use the built-in Terminal application Clone the following repository Open your chosen shell, and type: git clone https://github.com/mohitkr05/TelcoBootcamp.git If the clone is successful you'll see: $ git clone https://github.com/mohitkr05/TelcoBootcamp.git Cloning into 'TelcoBootcamp'... remote: Counting objects: 6, done. remote: Compressing objects: 100% (2/2), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (6/6), done.","title":"Checking History"},{"location":"excercises/04-vm-handson/","text":"Virtual Machine Hands-on","title":"Virtual Machine Hands-on"},{"location":"excercises/04-vm-handson/#virtual-machine-hands-on","text":"","title":"Virtual Machine Hands-on"},{"location":"excercises/Cloud-TelcoCloud/","text":"","title":"Cloud TelcoCloud"},{"location":"notes/01-introduction-telecom/","text":"Day 1 Introduction to Telecom Communication basics Communication is a process of transfer of information from Sender to Receiver over a communication medium Communication medium Verbal communication Written communication Electronics and Digital communication Visual communication Print media Broadcast media Gestures and body language Communication systems A communication system conveys information from its source to a destination. Capacity of a channel The most important question for a communication channel is the maximum rate at which it can transfer information. There is a theoretical maximum rate at which information passes error free over the channel, called the channel capacity C. The famous Hartley-Shannon Law states that the channel capacity C is given by: C=B*log(1+(S/N)) Where, - B is the bandwidth, - S/N is the signal-to-noise ratio. What is telecommunications? Telecommunications is the transmission of information over a distance. It encompasses a wide range of technologies, from simple telephone networks to complex fiber optic networks. What are the changes that telecommunications engineering tries to solve? General challenges in telecommunications engineering include: Performance Reliability Affordability Some specific challenges are - Increasing the capacity of telecommunications networks to handle more data traffic - Reducing the latency of telecommunications networks to improve the responsiveness of applications - Expanding the coverage of telecommunications networks to reach more people and places - Making telecommunications networks more secure and resilient to attacks - Reducing the cost of telecommunications services to make them more affordable for everyone What are the breakthroughs in telecommunications engineering? Year Innovation/Discovery or Invention Wikipedia article 1832 Samuel F. B. Morse invents the telegraph Telegraph 1876 Alexander Graham Bell invents the telephone Telephone 1887 Heinrich Hertz discovers radio waves Radio wave 1895 Guglielmo Marconi invents the radio Radio 1906 Lee De Forest invents the triode vacuum tube Triode 1920 The first commercial radio broadcast is made in Pittsburgh, Pennsylvania Commercial radio broadcasting 1927 Philo T. Farnsworth invents the television Television 1947 The transistor is invented at Bell Labs Transistor 1956 The integrated circuit is invented at Texas Instruments Integrated circuit 1969 The first ARPANET node is installed at UCLA ARPANET 1973 The first mobile phone call is made on a Motorola DynaTAC Mobile phone 1974 Vint Cerf and Bob Kahn develop the Transmission Control Protocol/Internet Protocol (TCP/IP) TCP/IP 1985 The first commercial cellular network is launched in the United States Cellular network 1989 Tim Berners-Lee invents the World Wide Web World Wide Web 1993 The first web browser, Mosaic, is released Web browser 1998 The first commercial satellite internet service is launched Satellite internet 2001 The first 3G cellular network is launched in Japan 3G 2009 The first 4G cellular network is launched in Sweden 4G 2019 The first 5G cellular network is launched in South Korea 5G This is just a small sample of the many breakthroughs in telecommunications engineering over the years. Telecommunications engineering has revolutionized the way we communicate and access information, and it continues to evolve at a rapid pace. Generations of telecom Primary Motivation Year Technology Details Link to Documentation To enable mobile communication 1946 0G Pre-cellular mobile radio telephone systems, such as Mobile Telephone System (MTS) and Advanced Mobile Telephone System (AMTS) Wikipedia article on 0G To improve the capacity and reliability of mobile communication 1979 1G Analog cellular networks, such as Advanced Mobile Phone System (AMPS) and Nordic Mobile Telephone (NMT) Wikipedia article on 1G To introduce digital technology to mobile communication, improving voice quality and data transmission rates 1991 2G Digital cellular networks, such as Global System for Mobile Communications (GSM) and Code Division Multiple Access (CDMA) Wikipedia article on 2G To provide high-speed data transmission for mobile devices 2001 3G Third-generation cellular networks, such as Universal Mobile Telecommunications System (UMTS) and CDMA2000 Wikipedia article on 3G To provide even higher-speed data transmission and lower latency for mobile devices, enabling new applications such as mobile video and mobile broadband 2009 4G Fourth-generation cellular networks, such as Long-Term Evolution (LTE) and WiMAX Wikipedia article on 4G To provide extremely high data rates, low latency, and massive connectivity for mobile devices and the Internet of Things (IoT), enabling new applications such as self-driving cars, augmented reality, and virtual reality 2019 5G Fifth-generation cellular networks Wikipedia article on 5G To further improve the performance and capabilities of 5G, enabling new applications such as 6G-enabled smart cities and factories 2030 (planned) 6G Sixth-generation cellular networks Wikipedia article on 6G Question What are some standards that help in adoption of the technologies across world?","title":"Day 1"},{"location":"notes/01-introduction-telecom/#day-1","text":"","title":"Day 1"},{"location":"notes/01-introduction-telecom/#introduction-to-telecom","text":"","title":"Introduction to Telecom"},{"location":"notes/01-introduction-telecom/#communication-basics","text":"Communication is a process of transfer of information from Sender to Receiver over a communication medium Communication medium Verbal communication Written communication Electronics and Digital communication Visual communication Print media Broadcast media Gestures and body language","title":"Communication basics"},{"location":"notes/01-introduction-telecom/#communication-systems","text":"A communication system conveys information from its source to a destination.","title":"Communication systems"},{"location":"notes/01-introduction-telecom/#capacity-of-a-channel","text":"The most important question for a communication channel is the maximum rate at which it can transfer information. There is a theoretical maximum rate at which information passes error free over the channel, called the channel capacity C. The famous Hartley-Shannon Law states that the channel capacity C is given by: C=B*log(1+(S/N)) Where, - B is the bandwidth, - S/N is the signal-to-noise ratio.","title":"Capacity of a channel"},{"location":"notes/01-introduction-telecom/#what-is-telecommunications","text":"Telecommunications is the transmission of information over a distance. It encompasses a wide range of technologies, from simple telephone networks to complex fiber optic networks.","title":"What is telecommunications?"},{"location":"notes/01-introduction-telecom/#what-are-the-changes-that-telecommunications-engineering-tries-to-solve","text":"General challenges in telecommunications engineering include: Performance Reliability Affordability Some specific challenges are - Increasing the capacity of telecommunications networks to handle more data traffic - Reducing the latency of telecommunications networks to improve the responsiveness of applications - Expanding the coverage of telecommunications networks to reach more people and places - Making telecommunications networks more secure and resilient to attacks - Reducing the cost of telecommunications services to make them more affordable for everyone","title":"What are the changes that telecommunications engineering tries to solve?"},{"location":"notes/01-introduction-telecom/#what-are-the-breakthroughs-in-telecommunications-engineering","text":"Year Innovation/Discovery or Invention Wikipedia article 1832 Samuel F. B. Morse invents the telegraph Telegraph 1876 Alexander Graham Bell invents the telephone Telephone 1887 Heinrich Hertz discovers radio waves Radio wave 1895 Guglielmo Marconi invents the radio Radio 1906 Lee De Forest invents the triode vacuum tube Triode 1920 The first commercial radio broadcast is made in Pittsburgh, Pennsylvania Commercial radio broadcasting 1927 Philo T. Farnsworth invents the television Television 1947 The transistor is invented at Bell Labs Transistor 1956 The integrated circuit is invented at Texas Instruments Integrated circuit 1969 The first ARPANET node is installed at UCLA ARPANET 1973 The first mobile phone call is made on a Motorola DynaTAC Mobile phone 1974 Vint Cerf and Bob Kahn develop the Transmission Control Protocol/Internet Protocol (TCP/IP) TCP/IP 1985 The first commercial cellular network is launched in the United States Cellular network 1989 Tim Berners-Lee invents the World Wide Web World Wide Web 1993 The first web browser, Mosaic, is released Web browser 1998 The first commercial satellite internet service is launched Satellite internet 2001 The first 3G cellular network is launched in Japan 3G 2009 The first 4G cellular network is launched in Sweden 4G 2019 The first 5G cellular network is launched in South Korea 5G This is just a small sample of the many breakthroughs in telecommunications engineering over the years. Telecommunications engineering has revolutionized the way we communicate and access information, and it continues to evolve at a rapid pace.","title":"What are the breakthroughs in telecommunications engineering?"},{"location":"notes/01-introduction-telecom/#generations-of-telecom","text":"Primary Motivation Year Technology Details Link to Documentation To enable mobile communication 1946 0G Pre-cellular mobile radio telephone systems, such as Mobile Telephone System (MTS) and Advanced Mobile Telephone System (AMTS) Wikipedia article on 0G To improve the capacity and reliability of mobile communication 1979 1G Analog cellular networks, such as Advanced Mobile Phone System (AMPS) and Nordic Mobile Telephone (NMT) Wikipedia article on 1G To introduce digital technology to mobile communication, improving voice quality and data transmission rates 1991 2G Digital cellular networks, such as Global System for Mobile Communications (GSM) and Code Division Multiple Access (CDMA) Wikipedia article on 2G To provide high-speed data transmission for mobile devices 2001 3G Third-generation cellular networks, such as Universal Mobile Telecommunications System (UMTS) and CDMA2000 Wikipedia article on 3G To provide even higher-speed data transmission and lower latency for mobile devices, enabling new applications such as mobile video and mobile broadband 2009 4G Fourth-generation cellular networks, such as Long-Term Evolution (LTE) and WiMAX Wikipedia article on 4G To provide extremely high data rates, low latency, and massive connectivity for mobile devices and the Internet of Things (IoT), enabling new applications such as self-driving cars, augmented reality, and virtual reality 2019 5G Fifth-generation cellular networks Wikipedia article on 5G To further improve the performance and capabilities of 5G, enabling new applications such as 6G-enabled smart cities and factories 2030 (planned) 6G Sixth-generation cellular networks Wikipedia article on 6G","title":"Generations of telecom"},{"location":"notes/01-introduction-telecom/#question","text":"What are some standards that help in adoption of the technologies across world?","title":"Question"},{"location":"notes/02-Cloud-TelcoCloud/","text":"Cloud Computing & Telco Cloud Checklist for today [ ] Understanding Cloud Computing [ ] Brief overview of Telco Cloud [ ] Linux and Open source [ ] Open source in Telecom [ ] Overview of the lectures and course platform [ ] Using Git and Github [ ] Setting up your dev environment using a VM [ ] Linux hands on exercise What we are trying to achieve Understanding Cloud Computing Definition of Cloud computing Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models [1] Essential Characteristics On-demand self-service . A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider. Broad network access . Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations). Resource pooling . The provider\u2019s computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. There is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter). Examples of resources include storage, processing, memory, and network bandwidth. Rapid elasticity . Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time. Measured service . Cloud systems automatically control and optimize resource use by leveraging a metering capability1 at some level of abstraction appropriate to the type of service (e.g.,storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service Service Models Software as a Service (SaaS) . The capability provided to the consumer is to use the provider\u2019s applications running on a cloud infrastructure2. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings. Platform as a Service (PaaS) . The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider.3 The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment. Infrastructure as a Service (IaaS) . The capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls). Deployment Models Private cloud . The cloud infrastructure is provisioned for exclusive use by a single organization comprising multiple consumers (e.g., business units). It may be owned, managed, and operated by the organization, a third party, or some combination of them, and it may exist on or off premises. Community cloud . The cloud infrastructure is provisioned for exclusive use by a specific community of consumers from organizations that have shared concerns (e.g., mission, security requirements, policy, and compliance considerations). It may be owned, managed, and operated by one or more of the organizations in the community, a third party, or some combination of them, and it may exist on or off premises. Public cloud . The cloud infrastructure is provisioned for open use by the general public. It may be owned, managed, and operated by a business, academic, or government organization, or some combination of them. It exists on the premises of the cloud provider. Hybrid cloud . The cloud infrastructure is a composition of two or more distinct cloud infrastructures (private, community, or public) that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load balancing between clouds) Challenges and benefits for Cloud Computing References 1 Brief overview of Telco Cloud Telco Cloud Overview: Telco cloud empowers service providers to operate networks like a cloud. Facilitates agile and efficient creation and deployment of services. Architecture: Telco cloud architectures seamlessly connect public, private, and edge cloud infrastructure. Enables DevOps-managed environments for digital service providers. Foundation: Built on a network functions virtualization (NFV)\u2013enabled platform. This platform, also known as telco cloud infrastructure, serves as the foundation. Abstraction Layer: Creates an abstraction layer for the telco network. Incorporates modern hardware resources. Network Functions: Supports modular cloud-native network functions (CNFs). Also accommodates virtualized network functions (VNFs). Software & Automation: Establishes a software foundation for creating and delivering next-generation services. This foundation requires automated orchestration to effectively manage CNFs and VNFs across all software-defined resources. Eliminates error-prone human activities, such as service provisioning. Reduces complexity and optimizes overall resource utilization. Orchestration & Intelligence Telco cloud automation oversees service life cycles across various clouds: Network cloud Hybrid cloud Edge cloud IT cloud (Software-Defined Data Center, or SDDC) Ensures integrated and consistent management. The last layer of a telco cloud is the intelligence layer. Focuses on service assurance and root-cause analysis. Quality of Service (QoS) Management: Telco cloud automation is responsible for instantiating and delivering services. It seamlessly meets different Quality of Service (QoS) requirements across network resources. Linux and Open source Introduction: Linux, an open-source operating system kernel, has become synonymous with the broader ethos of open source software development. Developed by Linus Torvalds in 1991, Linux represents a collaborative and community-driven approach to software creation. This operating system has not only revolutionized the landscape of computing but has also become a symbol of the open-source movement. Key Opensource events Year Achievements Remarks 1960s Development of the Multics operating system Multics laid the foundation for many of the concepts used in Unix, including the hierarchical file system, process forking, and inter-process communication. 1970s Creation of Unix by Ken Thompson and Dennis Ritchie at Bell Labs Unix became the dominant operating system for workstations and servers in the 1980s and 1990s. 1983 Release of the GNU General Public License (GPL) by Richard Stallman The GPL is a widely used open-source software license that requires all derivatives of the software to be released under the same license. 1991 Release of Linux by Linus Torvalds Linux is a free and open-source operating system that has become the dominant operating system for web servers and embedded devices. 1998 Apache Software Foundation The Apache Software Foundation is a non-profit organization that operates the development and maintenance of the open-source web server software Apache HTTP Server. 2004 Mozilla Foundation The Mozilla Foundation is a non-profit organization that operates the development and maintenance of the open-source web browser Mozilla Firefox. 2005 Linux Foundation The Linux Foundation is a non-profit organization that promotes and develops the Linux operating system. 2008 Open Source Initiative (OSI) The Open Source Initiative is a non-profit organization that promotes and develops open-source software. 2012 Release of the first Android smartphone Android is a free and open-source mobile operating system that is based on the Linux kernel. 2014 Release of the first smartwatch with Google Wear Google Wear is a free and open-source operating system for smartwatches that is based on the Linux kernel. 2016 Release of the first Chromebook with Google Chrome OS Chrome OS is a free and open-source operating system for Chromebooks that is based on the Linux kernel. 2018 Release of the first Raspberry Pi with a Linux operating system The Raspberry Pi is a small, inexpensive computer that has been very popular for education and hobbyists. Open Source Philosophy At its core, the open-source philosophy emphasizes transparency, collaboration, and the free exchange of ideas. Unlike proprietary software, open-source software allows users to view, modify, and distribute the source code. This not only promotes innovation but also empowers a global community of developers to contribute to the improvement of software, fostering a culture of shared knowledge. Linux Linux is an open-source operating system kernel that has played a transformative role in the world of computing. Originally created by Linus Torvalds in 1991, Linux has grown into a versatile and powerful platform known for its stability, security, and customization options. It is a Unix-like operating system, and its source code is freely available to the public, adhering to the principles of the open-source software development model. The impact of Linux extends across various domains, from servers and embedded systems to desktop environments. Renowned for its stability, Linux is a preferred choice for mission-critical systems and servers. Its open nature allows users to view, modify, and distribute the source code, fostering a collaborative community of developers and contributors worldwide. Linux has also influenced other projects and initiatives. The Linux Foundation, established in 2005, is a non-profit organization that promotes the development of the Linux operating system and provides support for open-source projects. Linux's kernel serves as the foundation for Android, the widely used mobile operating system, showcasing its adaptability. For more detailed information, you can refer to the Wikipedia page on Linux, where you will find comprehensive insights into its history, development, features, and the broader ecosystem it has influenced. Key Features of Linux: Stability and Reliability: Linux is renowned for its stability, making it a preferred choice for servers and critical systems. Security: Open source allows for constant scrutiny, making Linux a secure platform with rapid responses to vulnerabilities. Customizability: Users can tailor Linux distributions to suit specific needs, from lightweight systems to robust server configurations. Community Collaboration: The collaborative nature of open source is exemplified in the vast community of developers, contributors, and users working together to enhance Linux. Online forums, mailing lists, and collaborative platforms foster a sense of shared responsibility, where feedback and improvements come from a diverse pool of individuals. Impact on Computing: Linux has permeated various domains, from servers and embedded systems to desktop environments. Android, a widely used mobile operating system, is built on the Linux kernel. Additionally, Linux powers a significant portion of internet servers, contributing to the robustness of the World Wide Web. Open source in Telecom The telecommunications industry has witnessed a remarkable transformation in recent years, driven by the adoption of open-source technologies. This shift has brought about significant benefits, including increased flexibility, reduced costs, and enhanced interoperability. Open-source software has empowered telecom operators to break free from vendor lock-in and embrace a more collaborative approach to innovation. Key Open Source Projects in Telecom Several open-source projects have played a pivotal role in shaping the telecom landscape. These projects have fostered a vibrant community of developers and contributors, leading to the development of robust and scalable solutions. Here are a few notable examples: OPNFV (Open Platform for NFV) : OPNFV is an open-source platform that facilitates the deployment and management of Network Functions Virtualization (NFV) solutions. It provides a standardized framework for orchestrating and managing virtual network functions (VNFs) and NFV infrastructure. ONAP (Open Network Automation Platform) : ONAP is an open-source platform that automates the lifecycle management of telecom networks. It covers a wide range of network functions, including service provisioning, configuration, and performance monitoring. FRRouting (FRR) : FRRouting is an open-source routing protocol suite that provides a comprehensive set of routing protocols for various network environments. It supports a wide range of routing technologies, including BGP, OSPF, and ISIS. OpenDaylight (ODL) : OpenDaylight is an open-source SDN (Software-Defined Networking) controller that provides a platform for managing network resources through software. It enables network operators to centrally manage and control their network infrastructure, improving agility and efficiency. OpenRAN (Open Radio Access Network) : OpenRAN is an open-source initiative that promotes the development and deployment of interoperable and open-source software for radio access networks (RANs). It aims to reduce vendor lock-in and foster innovation in the RAN segment. Impact of Open Source on Telecom The adoption of open-source technologies has had a profound impact on the telecom industry. Here are some of the key benefits: Increased Flexibility and Agility: Open-source software allows telecom operators to customize and adapt solutions to their specific needs, without being constrained by vendor limitations. This flexibility enables them to respond quickly to changing market demands and technological advancements. Reduced Costs: Open-source software often comes with lower licensing fees or is entirely free to use, significantly reducing the capital expenditure (CAPEX) associated with software procurement. This cost-effectiveness is particularly important for telecom operators facing increasing cost pressures. Enhanced Interoperability: Open-source software promotes interoperability among different vendors and technologies, breaking down barriers to communication and enabling seamless integration of network components. This interoperability is crucial for building next-generation networks that can support diverse applications and services. Fostering Innovation: Open-source development fosters a collaborative environment where developers from different organizations can contribute to the improvement of software solutions. This collective effort accelerates innovation and leads to the creation of more advanced and feature-rich solutions. Strengthening Security: Open-source software undergoes rigorous community scrutiny, which helps identify and fix vulnerabilities quickly. This transparency enhances the overall security of open-source software solutions. Using Git and Github Setting up your dev environment using a VM The purpose of this VM will be to serve as your development machine which you can use to work on your projects. As this is consistent for everyone in the project, we are going to use the same version of operating system as a virtual machine. Download and install softwares using Chocolatey Download and install VirtualBox Download the Ubuntu ISO Image Install the virtual machine using VirtualBox Start the virtual machine Login to the VM","title":"Cloud Computing & Telco Cloud"},{"location":"notes/02-Cloud-TelcoCloud/#cloud-computing-telco-cloud","text":"","title":"Cloud Computing &amp; Telco Cloud"},{"location":"notes/02-Cloud-TelcoCloud/#checklist-for-today","text":"[ ] Understanding Cloud Computing [ ] Brief overview of Telco Cloud [ ] Linux and Open source [ ] Open source in Telecom [ ] Overview of the lectures and course platform [ ] Using Git and Github [ ] Setting up your dev environment using a VM [ ] Linux hands on exercise","title":"Checklist for today"},{"location":"notes/02-Cloud-TelcoCloud/#what-we-are-trying-to-achieve","text":"","title":"What we are trying to achieve"},{"location":"notes/02-Cloud-TelcoCloud/#understanding-cloud-computing","text":"","title":"Understanding Cloud Computing"},{"location":"notes/02-Cloud-TelcoCloud/#definition-of-cloud-computing","text":"Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models [1]","title":"Definition of Cloud computing"},{"location":"notes/02-Cloud-TelcoCloud/#essential-characteristics","text":"On-demand self-service . A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider. Broad network access . Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations). Resource pooling . The provider\u2019s computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. There is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter). Examples of resources include storage, processing, memory, and network bandwidth. Rapid elasticity . Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time. Measured service . Cloud systems automatically control and optimize resource use by leveraging a metering capability1 at some level of abstraction appropriate to the type of service (e.g.,storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service","title":"Essential Characteristics"},{"location":"notes/02-Cloud-TelcoCloud/#service-models","text":"Software as a Service (SaaS) . The capability provided to the consumer is to use the provider\u2019s applications running on a cloud infrastructure2. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings. Platform as a Service (PaaS) . The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider.3 The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment. Infrastructure as a Service (IaaS) . The capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).","title":"Service Models"},{"location":"notes/02-Cloud-TelcoCloud/#deployment-models","text":"Private cloud . The cloud infrastructure is provisioned for exclusive use by a single organization comprising multiple consumers (e.g., business units). It may be owned, managed, and operated by the organization, a third party, or some combination of them, and it may exist on or off premises. Community cloud . The cloud infrastructure is provisioned for exclusive use by a specific community of consumers from organizations that have shared concerns (e.g., mission, security requirements, policy, and compliance considerations). It may be owned, managed, and operated by one or more of the organizations in the community, a third party, or some combination of them, and it may exist on or off premises. Public cloud . The cloud infrastructure is provisioned for open use by the general public. It may be owned, managed, and operated by a business, academic, or government organization, or some combination of them. It exists on the premises of the cloud provider. Hybrid cloud . The cloud infrastructure is a composition of two or more distinct cloud infrastructures (private, community, or public) that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load balancing between clouds)","title":"Deployment Models"},{"location":"notes/02-Cloud-TelcoCloud/#challenges-and-benefits-for-cloud-computing","text":"References 1","title":"Challenges and benefits for Cloud Computing"},{"location":"notes/02-Cloud-TelcoCloud/#brief-overview-of-telco-cloud","text":"","title":"Brief overview of Telco Cloud"},{"location":"notes/02-Cloud-TelcoCloud/#telco-cloud-overview","text":"Telco cloud empowers service providers to operate networks like a cloud. Facilitates agile and efficient creation and deployment of services.","title":"Telco Cloud Overview:"},{"location":"notes/02-Cloud-TelcoCloud/#architecture","text":"Telco cloud architectures seamlessly connect public, private, and edge cloud infrastructure. Enables DevOps-managed environments for digital service providers.","title":"Architecture:"},{"location":"notes/02-Cloud-TelcoCloud/#foundation","text":"Built on a network functions virtualization (NFV)\u2013enabled platform. This platform, also known as telco cloud infrastructure, serves as the foundation.","title":"Foundation:"},{"location":"notes/02-Cloud-TelcoCloud/#abstraction-layer","text":"Creates an abstraction layer for the telco network. Incorporates modern hardware resources.","title":"Abstraction Layer:"},{"location":"notes/02-Cloud-TelcoCloud/#network-functions","text":"Supports modular cloud-native network functions (CNFs). Also accommodates virtualized network functions (VNFs).","title":"Network Functions:"},{"location":"notes/02-Cloud-TelcoCloud/#software-automation","text":"Establishes a software foundation for creating and delivering next-generation services. This foundation requires automated orchestration to effectively manage CNFs and VNFs across all software-defined resources. Eliminates error-prone human activities, such as service provisioning. Reduces complexity and optimizes overall resource utilization.","title":"Software &amp; Automation:"},{"location":"notes/02-Cloud-TelcoCloud/#orchestration-intelligence","text":"Telco cloud automation oversees service life cycles across various clouds: Network cloud Hybrid cloud Edge cloud IT cloud (Software-Defined Data Center, or SDDC) Ensures integrated and consistent management. The last layer of a telco cloud is the intelligence layer. Focuses on service assurance and root-cause analysis.","title":"Orchestration &amp; Intelligence"},{"location":"notes/02-Cloud-TelcoCloud/#quality-of-service-qos-management","text":"Telco cloud automation is responsible for instantiating and delivering services. It seamlessly meets different Quality of Service (QoS) requirements across network resources.","title":"Quality of Service (QoS) Management:"},{"location":"notes/02-Cloud-TelcoCloud/#linux-and-open-source","text":"","title":"Linux and Open source"},{"location":"notes/02-Cloud-TelcoCloud/#introduction","text":"Linux, an open-source operating system kernel, has become synonymous with the broader ethos of open source software development. Developed by Linus Torvalds in 1991, Linux represents a collaborative and community-driven approach to software creation. This operating system has not only revolutionized the landscape of computing but has also become a symbol of the open-source movement.","title":"Introduction:"},{"location":"notes/02-Cloud-TelcoCloud/#key-opensource-events","text":"Year Achievements Remarks 1960s Development of the Multics operating system Multics laid the foundation for many of the concepts used in Unix, including the hierarchical file system, process forking, and inter-process communication. 1970s Creation of Unix by Ken Thompson and Dennis Ritchie at Bell Labs Unix became the dominant operating system for workstations and servers in the 1980s and 1990s. 1983 Release of the GNU General Public License (GPL) by Richard Stallman The GPL is a widely used open-source software license that requires all derivatives of the software to be released under the same license. 1991 Release of Linux by Linus Torvalds Linux is a free and open-source operating system that has become the dominant operating system for web servers and embedded devices. 1998 Apache Software Foundation The Apache Software Foundation is a non-profit organization that operates the development and maintenance of the open-source web server software Apache HTTP Server. 2004 Mozilla Foundation The Mozilla Foundation is a non-profit organization that operates the development and maintenance of the open-source web browser Mozilla Firefox. 2005 Linux Foundation The Linux Foundation is a non-profit organization that promotes and develops the Linux operating system. 2008 Open Source Initiative (OSI) The Open Source Initiative is a non-profit organization that promotes and develops open-source software. 2012 Release of the first Android smartphone Android is a free and open-source mobile operating system that is based on the Linux kernel. 2014 Release of the first smartwatch with Google Wear Google Wear is a free and open-source operating system for smartwatches that is based on the Linux kernel. 2016 Release of the first Chromebook with Google Chrome OS Chrome OS is a free and open-source operating system for Chromebooks that is based on the Linux kernel. 2018 Release of the first Raspberry Pi with a Linux operating system The Raspberry Pi is a small, inexpensive computer that has been very popular for education and hobbyists.","title":"Key Opensource events"},{"location":"notes/02-Cloud-TelcoCloud/#open-source-philosophy","text":"At its core, the open-source philosophy emphasizes transparency, collaboration, and the free exchange of ideas. Unlike proprietary software, open-source software allows users to view, modify, and distribute the source code. This not only promotes innovation but also empowers a global community of developers to contribute to the improvement of software, fostering a culture of shared knowledge.","title":"Open Source Philosophy"},{"location":"notes/02-Cloud-TelcoCloud/#linux","text":"Linux is an open-source operating system kernel that has played a transformative role in the world of computing. Originally created by Linus Torvalds in 1991, Linux has grown into a versatile and powerful platform known for its stability, security, and customization options. It is a Unix-like operating system, and its source code is freely available to the public, adhering to the principles of the open-source software development model. The impact of Linux extends across various domains, from servers and embedded systems to desktop environments. Renowned for its stability, Linux is a preferred choice for mission-critical systems and servers. Its open nature allows users to view, modify, and distribute the source code, fostering a collaborative community of developers and contributors worldwide. Linux has also influenced other projects and initiatives. The Linux Foundation, established in 2005, is a non-profit organization that promotes the development of the Linux operating system and provides support for open-source projects. Linux's kernel serves as the foundation for Android, the widely used mobile operating system, showcasing its adaptability. For more detailed information, you can refer to the Wikipedia page on Linux, where you will find comprehensive insights into its history, development, features, and the broader ecosystem it has influenced.","title":"Linux"},{"location":"notes/02-Cloud-TelcoCloud/#key-features-of-linux","text":"Stability and Reliability: Linux is renowned for its stability, making it a preferred choice for servers and critical systems. Security: Open source allows for constant scrutiny, making Linux a secure platform with rapid responses to vulnerabilities. Customizability: Users can tailor Linux distributions to suit specific needs, from lightweight systems to robust server configurations.","title":"Key Features of Linux:"},{"location":"notes/02-Cloud-TelcoCloud/#community-collaboration","text":"The collaborative nature of open source is exemplified in the vast community of developers, contributors, and users working together to enhance Linux. Online forums, mailing lists, and collaborative platforms foster a sense of shared responsibility, where feedback and improvements come from a diverse pool of individuals.","title":"Community Collaboration:"},{"location":"notes/02-Cloud-TelcoCloud/#impact-on-computing","text":"Linux has permeated various domains, from servers and embedded systems to desktop environments. Android, a widely used mobile operating system, is built on the Linux kernel. Additionally, Linux powers a significant portion of internet servers, contributing to the robustness of the World Wide Web.","title":"Impact on Computing:"},{"location":"notes/02-Cloud-TelcoCloud/#open-source-in-telecom","text":"The telecommunications industry has witnessed a remarkable transformation in recent years, driven by the adoption of open-source technologies. This shift has brought about significant benefits, including increased flexibility, reduced costs, and enhanced interoperability. Open-source software has empowered telecom operators to break free from vendor lock-in and embrace a more collaborative approach to innovation.","title":"Open source in Telecom"},{"location":"notes/02-Cloud-TelcoCloud/#key-open-source-projects-in-telecom","text":"Several open-source projects have played a pivotal role in shaping the telecom landscape. These projects have fostered a vibrant community of developers and contributors, leading to the development of robust and scalable solutions. Here are a few notable examples: OPNFV (Open Platform for NFV) : OPNFV is an open-source platform that facilitates the deployment and management of Network Functions Virtualization (NFV) solutions. It provides a standardized framework for orchestrating and managing virtual network functions (VNFs) and NFV infrastructure. ONAP (Open Network Automation Platform) : ONAP is an open-source platform that automates the lifecycle management of telecom networks. It covers a wide range of network functions, including service provisioning, configuration, and performance monitoring. FRRouting (FRR) : FRRouting is an open-source routing protocol suite that provides a comprehensive set of routing protocols for various network environments. It supports a wide range of routing technologies, including BGP, OSPF, and ISIS. OpenDaylight (ODL) : OpenDaylight is an open-source SDN (Software-Defined Networking) controller that provides a platform for managing network resources through software. It enables network operators to centrally manage and control their network infrastructure, improving agility and efficiency. OpenRAN (Open Radio Access Network) : OpenRAN is an open-source initiative that promotes the development and deployment of interoperable and open-source software for radio access networks (RANs). It aims to reduce vendor lock-in and foster innovation in the RAN segment.","title":"Key Open Source Projects in Telecom"},{"location":"notes/02-Cloud-TelcoCloud/#impact-of-open-source-on-telecom","text":"The adoption of open-source technologies has had a profound impact on the telecom industry. Here are some of the key benefits: Increased Flexibility and Agility: Open-source software allows telecom operators to customize and adapt solutions to their specific needs, without being constrained by vendor limitations. This flexibility enables them to respond quickly to changing market demands and technological advancements. Reduced Costs: Open-source software often comes with lower licensing fees or is entirely free to use, significantly reducing the capital expenditure (CAPEX) associated with software procurement. This cost-effectiveness is particularly important for telecom operators facing increasing cost pressures. Enhanced Interoperability: Open-source software promotes interoperability among different vendors and technologies, breaking down barriers to communication and enabling seamless integration of network components. This interoperability is crucial for building next-generation networks that can support diverse applications and services. Fostering Innovation: Open-source development fosters a collaborative environment where developers from different organizations can contribute to the improvement of software solutions. This collective effort accelerates innovation and leads to the creation of more advanced and feature-rich solutions. Strengthening Security: Open-source software undergoes rigorous community scrutiny, which helps identify and fix vulnerabilities quickly. This transparency enhances the overall security of open-source software solutions.","title":"Impact of Open Source on Telecom"},{"location":"notes/02-Cloud-TelcoCloud/#using-git-and-github","text":"","title":"Using Git and Github"},{"location":"notes/02-Cloud-TelcoCloud/#setting-up-your-dev-environment-using-a-vm","text":"The purpose of this VM will be to serve as your development machine which you can use to work on your projects. As this is consistent for everyone in the project, we are going to use the same version of operating system as a virtual machine. Download and install softwares using Chocolatey Download and install VirtualBox Download the Ubuntu ISO Image Install the virtual machine using VirtualBox Start the virtual machine Login to the VM","title":"Setting up your dev environment using a VM"},{"location":"notes/03-linux/","text":"Linux Linux command Line hands on This tutorial is for a quick hands on demo for the class. A detailed hands on is based on the Linux Command Line handbook. Getting to know the system uname -a : Display system information including kernel version, hostname, and architecture. hostname : Show or set the system's hostname. lsb_release -a : Display distribution-specific information. cat /etc/os-release : View information about the operating system. cat /proc/version : Show Linux kernel version and information. uptime : Display how long the system has been running. date : Show the current date and time. cal : Display a calendar. w : Show who is logged in and what they are doing. whoami : Display the username of the current user. id : Display user and group information. Understanding the filesystem Basic Filesystem Structure: / (Root Directory): The top-level directory of the filesystem hierarchy. /bin: Essential system binaries (commands) needed for system repair and recovery. /sbin: System binaries specifically used for system administration tasks. /etc: Configuration files and directories. /home: Home directories for users. /var: Variable data, such as logs, spool files, and temporary files. /tmp: Temporary files. /usr: User-related programs and files. /lib and /lib64: Essential shared libraries needed for system boot. /dev: Device files representing hardware devices. /proc: A virtual filesystem that provides information about processes and kernel parameters. File system navigation pwd : Print the current working directory. ls : List directory contents. cd : Change directory. cp : Copy files or directories. mv : Move or rename files or directories. rm : Remove files or directories. file : Determine file type. stat : Display detailed file or filesystem status. du : Estimate file space usage. df : Display disk space usage. cat : Concatenate and display file content. File permissions and ownerships File permissions and ownership are crucial aspects of Linux file system security. They determine who can access, modify, or execute files and directories. Each file and directory has an associated owner, group, and a set of permissions for three categories of users: owner, group, and others. File Permissions: Read (r): Allows the reading of a file's contents or viewing a directory's listing. Write (w): Permits the modification of a file's contents or the creation and deletion of files within a directory. Execute (x): Grants the ability to execute a file or traverse a directory. File Ownership: Owner: The user who owns the file. This user has special permissions to read, write, and execute the file. Group: A set of users who share the same permissions on the file. Files can belong to a user and a group. Commands for File Permissions and Ownership: chmod - Change Mode - Used to change file or directory permissions. chmod permissions filename chmod u+rw filename chmod o-w filename chown - Change Owner - Used to change the owner of a file or directory. chown user:group filename chown john filename chgrp - Change Group - Used to change the group ownership of a file or directory. chgrp groupname filename chgrp staff filename Checking the hardware df -h : Display disk space usage in a human-readable format. free -m : Display the amount of free and used memory in megabytes. lscpu : Display CPU information. lsblk : List block devices (disks and partitions). lshw : List hardware information. lspci : Display PCI devices. lsusb : Display USB devices. ifconfig or ip a : Show network interfaces and their configurations. route -n : Display the routing table. cat /etc/resolv.conf : View DNS configuration. Processes ps aux : Display information about all running processes. top : Display dynamic real-time information about running processes. systemctl list-units --type=service : List all active services. Understanding users and Groups useradd - To add a user - e.g. sudo useradd username userdel - To delete a user - e.g. sudo userdel username groupadd - To add a group - e.g. sudo groupadd groupname groupdel - To delete a group - e.g. sudo groupdel groupname usermod - Add user to group - e.g. sudo usermod -a -G groupname username su - Switch user - e.g. su username Project Basic Linux system administrator The objective of this excercise is to understand how system administrators work in Linux Change the hostname of the system nms01 . Validate the system configuration using command line commands. Hostname Type of shell Home directory List of users defined List of groups defined List of users in each group Disk space Memory usage Hardware attached Type of distribution and version Linux kernel version IP address Reachability to internet Timezone Check the root directory and print out the nested directory structure What is the type of package manager used? Update the system to latest version.","title":"Linux"},{"location":"notes/03-linux/#linux","text":"","title":"Linux"},{"location":"notes/03-linux/#linux-command-line-hands-on","text":"This tutorial is for a quick hands on demo for the class. A detailed hands on is based on the Linux Command Line handbook.","title":"Linux command Line hands on"},{"location":"notes/03-linux/#getting-to-know-the-system","text":"uname -a : Display system information including kernel version, hostname, and architecture. hostname : Show or set the system's hostname. lsb_release -a : Display distribution-specific information. cat /etc/os-release : View information about the operating system. cat /proc/version : Show Linux kernel version and information. uptime : Display how long the system has been running. date : Show the current date and time. cal : Display a calendar. w : Show who is logged in and what they are doing. whoami : Display the username of the current user. id : Display user and group information.","title":"Getting to know the system"},{"location":"notes/03-linux/#understanding-the-filesystem","text":"","title":"Understanding the filesystem"},{"location":"notes/03-linux/#basic-filesystem-structure","text":"/ (Root Directory): The top-level directory of the filesystem hierarchy. /bin: Essential system binaries (commands) needed for system repair and recovery. /sbin: System binaries specifically used for system administration tasks. /etc: Configuration files and directories. /home: Home directories for users. /var: Variable data, such as logs, spool files, and temporary files. /tmp: Temporary files. /usr: User-related programs and files. /lib and /lib64: Essential shared libraries needed for system boot. /dev: Device files representing hardware devices. /proc: A virtual filesystem that provides information about processes and kernel parameters.","title":"Basic Filesystem Structure:"},{"location":"notes/03-linux/#file-system-navigation","text":"pwd : Print the current working directory. ls : List directory contents. cd : Change directory. cp : Copy files or directories. mv : Move or rename files or directories. rm : Remove files or directories. file : Determine file type. stat : Display detailed file or filesystem status. du : Estimate file space usage. df : Display disk space usage. cat : Concatenate and display file content.","title":"File system navigation"},{"location":"notes/03-linux/#file-permissions-and-ownerships","text":"File permissions and ownership are crucial aspects of Linux file system security. They determine who can access, modify, or execute files and directories. Each file and directory has an associated owner, group, and a set of permissions for three categories of users: owner, group, and others.","title":"File permissions and ownerships"},{"location":"notes/03-linux/#file-permissions","text":"Read (r): Allows the reading of a file's contents or viewing a directory's listing. Write (w): Permits the modification of a file's contents or the creation and deletion of files within a directory. Execute (x): Grants the ability to execute a file or traverse a directory.","title":"File Permissions:"},{"location":"notes/03-linux/#file-ownership","text":"Owner: The user who owns the file. This user has special permissions to read, write, and execute the file. Group: A set of users who share the same permissions on the file. Files can belong to a user and a group.","title":"File Ownership:"},{"location":"notes/03-linux/#commands-for-file-permissions-and-ownership","text":"chmod - Change Mode - Used to change file or directory permissions. chmod permissions filename chmod u+rw filename chmod o-w filename chown - Change Owner - Used to change the owner of a file or directory. chown user:group filename chown john filename chgrp - Change Group - Used to change the group ownership of a file or directory. chgrp groupname filename chgrp staff filename","title":"Commands for File Permissions and Ownership:"},{"location":"notes/03-linux/#checking-the-hardware","text":"df -h : Display disk space usage in a human-readable format. free -m : Display the amount of free and used memory in megabytes. lscpu : Display CPU information. lsblk : List block devices (disks and partitions). lshw : List hardware information. lspci : Display PCI devices. lsusb : Display USB devices. ifconfig or ip a : Show network interfaces and their configurations. route -n : Display the routing table. cat /etc/resolv.conf : View DNS configuration.","title":"Checking the hardware"},{"location":"notes/03-linux/#processes","text":"ps aux : Display information about all running processes. top : Display dynamic real-time information about running processes. systemctl list-units --type=service : List all active services.","title":"Processes"},{"location":"notes/03-linux/#understanding-users-and-groups","text":"useradd - To add a user - e.g. sudo useradd username userdel - To delete a user - e.g. sudo userdel username groupadd - To add a group - e.g. sudo groupadd groupname groupdel - To delete a group - e.g. sudo groupdel groupname usermod - Add user to group - e.g. sudo usermod -a -G groupname username su - Switch user - e.g. su username","title":"Understanding users and Groups"},{"location":"notes/03-linux/#project","text":"","title":"Project"},{"location":"notes/03-linux/#basic-linux-system-administrator","text":"The objective of this excercise is to understand how system administrators work in Linux Change the hostname of the system nms01 . Validate the system configuration using command line commands. Hostname Type of shell Home directory List of users defined List of groups defined List of users in each group Disk space Memory usage Hardware attached Type of distribution and version Linux kernel version IP address Reachability to internet Timezone Check the root directory and print out the nested directory structure What is the type of package manager used? Update the system to latest version.","title":"Basic Linux system administrator"},{"location":"notes/04-linux-networking/","text":"Linux networking explained Network Interface Cards (NICs) NICs are physical components that allow a Linux system to connect to a network. Each NIC has a unique MAC address, which is used for network identification. Common NIC types include Ethernet, Wi-Fi, and fiber optics. How to check NICs in Linux ifconfig enp0s3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255 inet6 xxxx::xxxxx:xxxx:xxxx:3xxx prefixlen 64 scopeid 0x20<link> ether xx:xx:xx:xx:xx:xx txqueuelen 1000 (Ethernet) RX packets 93930 bytes 136962512 (136.9 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 16935 bytes 1228243 (1.2 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 In the above outpuit the following details are captured enp0s3 : NIC name flags=4163<UP,BROADCAST,RUNNING,MULTICAST> : NIC flags mtu 1500 : Maximum transmission unit inet 10.0.2.15 : IP address netmask 255.255.255.0 : Subnet mask inet6 xxxx::xx:xxxx:xxxx:xxxx - IPv6 Address xxxx::xx:xxxx:xxxx:xxxx - IPv6 link-local address . prefixlen 64 - The prefix length for the IPv6 address is 64 bits, which is a common configuration for local networks. scopeid 0x20<link> - Scope ID: This indicates the scope of the address, which is link-local (only valid in the local network segment). MAC Address: ether 02:d2:x6:x7:xd:x7 - The hardware MAC address of the network interface txqueuelen 1000 : The transmit queue length is set to 1000. RX (Recieve) Statistics: RX packets 93930: The number of received packets is 93,930. bytes 136962512 (136.9 MB): The total size of received data is approximately 136.9 MB. RX errors 0: No receive errors. dropped 0: No dropped packets. overruns 0: No overruns. frame 0: No frame errors. TX (Transmit) Statistics: TX packets 16935: The number of transmitted packets is 16,935. bytes 1228243 (1.2 MB): The total size of transmitted data is approximately 1.2 MB. TX errors 0: No transmit errors. dropped 0: No dropped packets. overruns 0: No overruns. carrier 0: No carrier errors. collisions 0: No collisions. Validate arps The ip to mac resolution is handled by the layer two broadcast protocol arp. The arp table can be displayed with the arp tool. The screenshot below shows the list of computers that this computer recently communicated with # arp Address HWtype HWaddress Flags Mask Iface 10.0.2.3 ether ----:--:--:--:--:-- C enp0s3 _gateway ether ----:--:--:--:--:-- C enp0s3 Validate the routes View the route table with netstat -r or route # route Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 100 0 0 enp0s3 10.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 enp0s3 _gateway 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s3 10.0.2.3 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s3 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s8 TCP/IP Protocol Stack TCP/IP is the standard networking protocol suite used in Linux systems. It consists of a layered model, with each layer responsible for specific tasks. Key layers include the Link Layer, Internet Layer, Transport Layer, and Application Layer. IP Addressing IP addresses are unique identifiers assigned to devices on a network. IPv4 addresses are 32-bit numbers, while IPv6 addresses are 128-bit numbers. IP addresses are used to route data packets between devices on the network. Subnetting Subnetting is the process of dividing a network into smaller subnets. Subnets are created by dividing the IP address into a network portion and a host portion. Subnetting improves network performance and security. Network Routing Routing is the process of directing data packets to their intended destinations. Routers are devices that forward data packets based on their IP addresses. Linux systems can act as routers, forwarding traffic between networks. Network Services Linux systems can provide a variety of network services, such as web servers, email servers, and file servers. These services allow Linux systems to share resources and communicate with other devices on the network. Network Security Network security is essential for protecting Linux systems from unauthorized access and attacks. Firewalls, intrusion detection systems (IDS), and encryption are common security measures. Linux systems offer various security tools and configurations to protect against network threats. Network Troubleshooting Network troubleshooting involves identifying and resolving network problems. Common troubleshooting tools include ping, netstat, and ifconfig. Understanding network protocols and concepts is crucial for effective troubleshooting. Network Configuration Network configuration involves setting up and managing network settings on Linux systems. This includes configuring IP addresses, network interfaces, and routing tables. Network configuration tools vary depending on the Linux distribution. Network Monitoring Network monitoring involves tracking and analyzing network performance and health. Monitoring tools provide insights into network traffic, resource utilization, and potential issues. Continuous network monitoring helps maintain network stability and identify potential problems early on. Sockets Sockets are fundamental building blocks of network communication in Linux, providing a standardized way for processes to exchange data across a network. They act as endpoints for communication, allowing processes to send and receive data over a network connection. Sockets are essential for implementing various network applications, such as web servers, file transfer clients, and chat programs. Linux supports two primary types of sockets: Stream sockets: These sockets provide a reliable, byte-oriented, and ordered data transmission. They maintain a connection between the communicating parties and ensure data delivery in the correct sequence. Stream sockets are commonly used for protocols like TCP, which is the foundation for many network applications like web browsing and file transfers. Datagram sockets: These sockets provide an unreliable, message-oriented, and unordered data transmission. They send data as individual packets, without maintaining a connection between the communicating parties. Datagram sockets are suitable for protocols like UDP, which is often used for applications where speed and low overhead are prioritized over reliable delivery, such as real-time streaming or gaming. Socket Addressing: Every socket has a unique address that identifies it on the network. This address consists of two components: IP address: The IP address identifies the network node where the socket resides. It's a 32-bit or 128-bit number for IPv4 and IPv6, respectively. Port number: The port number identifies a specific service or application associated with the socket. Port numbers are 16-bit integers ranging from 0 to 65535. Well-known services, such as web servers and email servers, have standardized port numbers assigned to them. Socket Operations: Sockets provide various operations for managing communication, including: Creating a socket: The `socket() system call creates a socket and assigns it a unique descriptor. Binding a socket: The bind() system call associates a socket with a specific IP address and port number. Connecting a socket: The connect() system call establishes a connection with a remote socket identified by its IP address and port number. Sending data: The send() system call sends data from the local socket to the connected remote socket. Receiving data: The recv() system call receives data from the connected remote socket into the local socket buffer Closing a socket: The close() system call terminates the connection and releases the socket resources.","title":"Linux networking explained"},{"location":"notes/04-linux-networking/#linux-networking-explained","text":"","title":"Linux networking explained"},{"location":"notes/04-linux-networking/#network-interface-cards-nics","text":"NICs are physical components that allow a Linux system to connect to a network. Each NIC has a unique MAC address, which is used for network identification. Common NIC types include Ethernet, Wi-Fi, and fiber optics.","title":"Network Interface Cards (NICs)"},{"location":"notes/04-linux-networking/#how-to-check-nics-in-linux","text":"ifconfig enp0s3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255 inet6 xxxx::xxxxx:xxxx:xxxx:3xxx prefixlen 64 scopeid 0x20<link> ether xx:xx:xx:xx:xx:xx txqueuelen 1000 (Ethernet) RX packets 93930 bytes 136962512 (136.9 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 16935 bytes 1228243 (1.2 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 In the above outpuit the following details are captured enp0s3 : NIC name flags=4163<UP,BROADCAST,RUNNING,MULTICAST> : NIC flags mtu 1500 : Maximum transmission unit inet 10.0.2.15 : IP address netmask 255.255.255.0 : Subnet mask inet6 xxxx::xx:xxxx:xxxx:xxxx - IPv6 Address xxxx::xx:xxxx:xxxx:xxxx - IPv6 link-local address . prefixlen 64 - The prefix length for the IPv6 address is 64 bits, which is a common configuration for local networks. scopeid 0x20<link> - Scope ID: This indicates the scope of the address, which is link-local (only valid in the local network segment). MAC Address: ether 02:d2:x6:x7:xd:x7 - The hardware MAC address of the network interface txqueuelen 1000 : The transmit queue length is set to 1000. RX (Recieve) Statistics: RX packets 93930: The number of received packets is 93,930. bytes 136962512 (136.9 MB): The total size of received data is approximately 136.9 MB. RX errors 0: No receive errors. dropped 0: No dropped packets. overruns 0: No overruns. frame 0: No frame errors. TX (Transmit) Statistics: TX packets 16935: The number of transmitted packets is 16,935. bytes 1228243 (1.2 MB): The total size of transmitted data is approximately 1.2 MB. TX errors 0: No transmit errors. dropped 0: No dropped packets. overruns 0: No overruns. carrier 0: No carrier errors. collisions 0: No collisions.","title":"How to check NICs in Linux"},{"location":"notes/04-linux-networking/#validate-arps","text":"The ip to mac resolution is handled by the layer two broadcast protocol arp. The arp table can be displayed with the arp tool. The screenshot below shows the list of computers that this computer recently communicated with # arp Address HWtype HWaddress Flags Mask Iface 10.0.2.3 ether ----:--:--:--:--:-- C enp0s3 _gateway ether ----:--:--:--:--:-- C enp0s3","title":"Validate arps"},{"location":"notes/04-linux-networking/#validate-the-routes","text":"View the route table with netstat -r or route # route Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 100 0 0 enp0s3 10.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 enp0s3 _gateway 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s3 10.0.2.3 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s3 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s8 TCP/IP Protocol Stack TCP/IP is the standard networking protocol suite used in Linux systems. It consists of a layered model, with each layer responsible for specific tasks. Key layers include the Link Layer, Internet Layer, Transport Layer, and Application Layer. IP Addressing IP addresses are unique identifiers assigned to devices on a network. IPv4 addresses are 32-bit numbers, while IPv6 addresses are 128-bit numbers. IP addresses are used to route data packets between devices on the network. Subnetting Subnetting is the process of dividing a network into smaller subnets. Subnets are created by dividing the IP address into a network portion and a host portion. Subnetting improves network performance and security. Network Routing Routing is the process of directing data packets to their intended destinations. Routers are devices that forward data packets based on their IP addresses. Linux systems can act as routers, forwarding traffic between networks. Network Services Linux systems can provide a variety of network services, such as web servers, email servers, and file servers. These services allow Linux systems to share resources and communicate with other devices on the network. Network Security Network security is essential for protecting Linux systems from unauthorized access and attacks. Firewalls, intrusion detection systems (IDS), and encryption are common security measures. Linux systems offer various security tools and configurations to protect against network threats. Network Troubleshooting Network troubleshooting involves identifying and resolving network problems. Common troubleshooting tools include ping, netstat, and ifconfig. Understanding network protocols and concepts is crucial for effective troubleshooting. Network Configuration Network configuration involves setting up and managing network settings on Linux systems. This includes configuring IP addresses, network interfaces, and routing tables. Network configuration tools vary depending on the Linux distribution. Network Monitoring Network monitoring involves tracking and analyzing network performance and health. Monitoring tools provide insights into network traffic, resource utilization, and potential issues. Continuous network monitoring helps maintain network stability and identify potential problems early on. Sockets Sockets are fundamental building blocks of network communication in Linux, providing a standardized way for processes to exchange data across a network. They act as endpoints for communication, allowing processes to send and receive data over a network connection. Sockets are essential for implementing various network applications, such as web servers, file transfer clients, and chat programs. Linux supports two primary types of sockets: Stream sockets: These sockets provide a reliable, byte-oriented, and ordered data transmission. They maintain a connection between the communicating parties and ensure data delivery in the correct sequence. Stream sockets are commonly used for protocols like TCP, which is the foundation for many network applications like web browsing and file transfers. Datagram sockets: These sockets provide an unreliable, message-oriented, and unordered data transmission. They send data as individual packets, without maintaining a connection between the communicating parties. Datagram sockets are suitable for protocols like UDP, which is often used for applications where speed and low overhead are prioritized over reliable delivery, such as real-time streaming or gaming. Socket Addressing: Every socket has a unique address that identifies it on the network. This address consists of two components: IP address: The IP address identifies the network node where the socket resides. It's a 32-bit or 128-bit number for IPv4 and IPv6, respectively. Port number: The port number identifies a specific service or application associated with the socket. Port numbers are 16-bit integers ranging from 0 to 65535. Well-known services, such as web servers and email servers, have standardized port numbers assigned to them. Socket Operations: Sockets provide various operations for managing communication, including: Creating a socket: The `socket() system call creates a socket and assigns it a unique descriptor. Binding a socket: The bind() system call associates a socket with a specific IP address and port number. Connecting a socket: The connect() system call establishes a connection with a remote socket identified by its IP address and port number. Sending data: The send() system call sends data from the local socket to the connected remote socket. Receiving data: The recv() system call receives data from the connected remote socket into the local socket buffer Closing a socket: The close() system call terminates the connection and releases the socket resources.","title":"Validate the routes"},{"location":"notes/05-git/","text":"Git & GitOps for Telecom Engineers What is Git Git is a distributed version control system (DVCS) designed to handle everything from small to very large projects with speed and efficiency. It was created by Linus Torvalds in 2005 to manage the development of the Linux kernel. Git is widely used for source code management (SCM) and is the most popular version control system in the world. Here are some key concepts and features of Git: Version Control : Git allows developers to track changes in their codebase over time. Each change is recorded, and developers can navigate through different versions of the code. Distributed : Git is a distributed version control system, meaning that every developer has a complete copy of the repository on their local machine. This allows for offline work and faster operations. Branching and Merging : Git makes it easy to create branches for parallel development and merge them back into the main codebase. This is crucial for collaborative development, enabling multiple developers to work on different features simultaneously. History Tracking : Git maintains a detailed history of changes, making it possible to trace back and understand how the code evolved. This is valuable for debugging and auditing purposes. Staging Area : Git uses a staging area, also known as the index, where changes can be reviewed and selectively committed before making them a permanent part of the project history. What is GitOps GitOps is a set of practices that use Git as the single source of truth for managing infrastructure and application delivery. It extends the principles of Git to operations, promoting a declarative approach to infrastructure and application management. The core idea is to use Git repositories not only for source code but also as the source of truth for the desired state of the entire system. Key concepts of GitOps: Declarative Configuration : In GitOps, the desired state of the system is declared in a Git repository. This includes both application code and infrastructure configuration. Changes to the system are made by updating the Git repository with the desired changes. Versioned Configuration : Git provides version control for the entire system configuration. This allows for easy rollback to a previous state and provides an audit trail of changes over time. Automation : GitOps relies heavily on automation to continuously monitor the Git repository for changes and automatically apply those changes to the target environment. Continuous Deployment (CD) pipelines are often used to automate the deployment process. Reconciliation : The actual state of the system is continuously compared with the declared state in the Git repository, and any deviations are automatically corrected. This ensures that the system is always in the desired state. Infrastructure as Code Infrastructure as Code (IaC) for Network Ops refers to the practice of using code to automate the provisioning, configuration, and management of network infrastructure in a telecom environment. This approach brings several benefits to the telecom industry, improving efficiency, reliability, and scalability. Here's how IaC for Network Ops helps in the telecom sector: Automation and Efficiency: Rapid Deployment: IaC allows for the rapid and consistent deployment of network infrastructure. Telecom operators can automate the provisioning of networking components, reducing the time it takes to set up new services or make changes to existing ones. Consistency: IaC ensures that the network configuration is consistent across different environments. This reduces the chances of errors caused by manual configuration and enhances overall network reliability. Scalability: Elasticity: Telecom networks often need to scale up or down based on demand. IaC enables dynamic scaling by automatically adjusting the network infrastructure to accommodate changes in traffic or service requirements. Version Control and Rollbacks: Versioning: IaC tools enable version control for network configurations. This means that changes to the network can be tracked over time, and different versions can be managed. Rollback Capabilities: In case of issues or failures, IaC allows for easy rollback to a previous version of the network configuration. This helps in quickly reverting to a known and working state. Collaboration and Documentation: Collaboration: IaC facilitates collaboration among network operations teams by allowing them to work on the same codebase. This promotes better communication and coordination in managing network infrastructure. Documentation: The code itself serves as documentation for the network configuration. This makes it easier for teams to understand how the network is set up and configured, improving knowledge sharing and troubleshooting. Policy Compliance: Enforcement of Policies: IaC can be used to enforce network policies and compliance standards. This ensures that the network configurations adhere to security, regulatory, and operational requirements. Testing and Validation: Automated Testing: IaC allows for the creation of automated tests to validate network configurations before deployment. This helps catch potential issues early in the development process, reducing the likelihood of errors in the live network. Cost Optimization: Resource Optimization: IaC can help optimize resource usage by dynamically adjusting network resources based on demand. This can lead to cost savings by ensuring that resources are allocated efficiently. GitOps in Telecom Repository Structure : Create a Git repository to store configuration files for network equipment. This repository becomes the single source of truth for the desired state of the network. Organize the repository with directories for different types of equipment, such as routers, switches, and firewalls. Infrastructure as Code (IaC) : Represent network configurations as code using Infrastructure as Code (IaC) principles. This means defining network configurations in a declarative format that can be version-controlled in Git. Version Control : Leverage Git's version control capabilities to track changes to network configurations over time. Each commit should represent a specific configuration state, making it easy to roll back to previous configurations if needed. Branching Strategy : Implement a branching strategy in Git to manage different environments or stages of the network (e.g., development, testing, production). This allows for parallel development and testing of configurations. Continuous Integration/Continuous Deployment (CI/CD) : Set up CI/CD pipelines to automate the testing and deployment of network configurations. Changes pushed to the Git repository trigger these pipelines, ensuring that configurations are tested before being applied to the actual network equipment. Git Hooks for Automation : Use Git hooks to trigger automation scripts whenever changes are pushed to the repository. These scripts can validate configurations, generate documentation, and initiate deployment processes. Secrets Management : Implement secure handling of sensitive information, such as device credentials and access keys. Use tools or practices for managing secrets securely within the GitOps workflow. Reconciliation : Employ reconciliation mechanisms to continuously compare the actual state of network equipment with the declared state in the Git repository. Automated tools can be used to detect and correct any discrepancies. Monitoring and Auditing : Integrate monitoring tools to keep track of the network's operational status. Implement logging and auditing mechanisms to trace changes and activities related to network configurations. Collaboration and Code Review : Facilitate collaboration among network engineers by enabling them to review and comment on proposed changes through Git pull requests. This helps ensure that configurations are thoroughly reviewed before being merged. Documentation : Use the Git repository not only for configurations but also for storing documentation related to network equipment. This documentation can include network diagrams, change logs, and other relevant information. Working with Git Configure Git: git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\" Initialize a Git Repository git init Clone a Repository: git clone <repository_url> Creates a copy of a remote repository on your local machine. Working with Changes Check Status: git status Shows the status of changes as untracked, modified, or staged. Stage Changes: git add <file(s)> Adds changes to the staging area in preparation for a commit. Commit Changes: git commit -m \"Commit message\" Records staged changes with a descriptive commit message. Branching and Merging Create a Branch: git branch <branch_name> Creates a new branch. Switch Branch: git checkout <branch_name> Switches to the specified branch. Merge Branch: `bash git merge <branch_name> Combines changes from the specified branch into the current branch. Working with Remotes Add a Remote Repository: git remote add <remote_name> <remote_url> Adds a remote repository. Push Changes to a Remote Repository: git push <remote_name> <branch_name> Pushes committed changes to a remote repository. Pull Changes from a Remote Repository: git pull <remote_name> <branch_name> Fetches and merges changes from a remote repository. Checking History View Commit History: git log Displays a log of commits. Show Changes in a Commit: git show <commit_hash> Displays the changes introduced by a specific commit. Downloading and installing Git : If you don't already have Git installed, you can download Git at www.git-scm.com. If you need additional assistance installing Git, you can find more information in the ProGit chapter on installing Git. Now is a good time to create a shortcut to the command-line application you will want to use with Git: If you are working on Windows, It is highly recommended to use Git Bash which is installed with the Git package, so that you can follow along with the facilitator who will be using Bash.If you are working on macOS or another Unix-like system, you can use the built-in Terminal application Clone the following repository Open your chosen shell, and type: git clone https://github.com/mohitkr05/TelcoBootcamp.git If the clone is successful you'll see: $ git clone https://github.com/mohitkr05/TelcoBootcamp.git Cloning into 'TelcoBootcamp'... remote: Counting objects: 6, done. remote: Compressing objects: 100% (2/2), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (6/6), done. Assignment Excercise - Git Read the Pro Git Ebook Watch the Linux command line video","title":"Git & GitOps for Telecom Engineers"},{"location":"notes/05-git/#git-gitops-for-telecom-engineers","text":"","title":"Git &amp; GitOps for Telecom Engineers"},{"location":"notes/05-git/#what-is-git","text":"Git is a distributed version control system (DVCS) designed to handle everything from small to very large projects with speed and efficiency. It was created by Linus Torvalds in 2005 to manage the development of the Linux kernel. Git is widely used for source code management (SCM) and is the most popular version control system in the world. Here are some key concepts and features of Git: Version Control : Git allows developers to track changes in their codebase over time. Each change is recorded, and developers can navigate through different versions of the code. Distributed : Git is a distributed version control system, meaning that every developer has a complete copy of the repository on their local machine. This allows for offline work and faster operations. Branching and Merging : Git makes it easy to create branches for parallel development and merge them back into the main codebase. This is crucial for collaborative development, enabling multiple developers to work on different features simultaneously. History Tracking : Git maintains a detailed history of changes, making it possible to trace back and understand how the code evolved. This is valuable for debugging and auditing purposes. Staging Area : Git uses a staging area, also known as the index, where changes can be reviewed and selectively committed before making them a permanent part of the project history.","title":"What is Git"},{"location":"notes/05-git/#what-is-gitops","text":"GitOps is a set of practices that use Git as the single source of truth for managing infrastructure and application delivery. It extends the principles of Git to operations, promoting a declarative approach to infrastructure and application management. The core idea is to use Git repositories not only for source code but also as the source of truth for the desired state of the entire system. Key concepts of GitOps: Declarative Configuration : In GitOps, the desired state of the system is declared in a Git repository. This includes both application code and infrastructure configuration. Changes to the system are made by updating the Git repository with the desired changes. Versioned Configuration : Git provides version control for the entire system configuration. This allows for easy rollback to a previous state and provides an audit trail of changes over time. Automation : GitOps relies heavily on automation to continuously monitor the Git repository for changes and automatically apply those changes to the target environment. Continuous Deployment (CD) pipelines are often used to automate the deployment process. Reconciliation : The actual state of the system is continuously compared with the declared state in the Git repository, and any deviations are automatically corrected. This ensures that the system is always in the desired state.","title":"What is GitOps"},{"location":"notes/05-git/#infrastructure-as-code","text":"Infrastructure as Code (IaC) for Network Ops refers to the practice of using code to automate the provisioning, configuration, and management of network infrastructure in a telecom environment. This approach brings several benefits to the telecom industry, improving efficiency, reliability, and scalability. Here's how IaC for Network Ops helps in the telecom sector: Automation and Efficiency: Rapid Deployment: IaC allows for the rapid and consistent deployment of network infrastructure. Telecom operators can automate the provisioning of networking components, reducing the time it takes to set up new services or make changes to existing ones. Consistency: IaC ensures that the network configuration is consistent across different environments. This reduces the chances of errors caused by manual configuration and enhances overall network reliability. Scalability: Elasticity: Telecom networks often need to scale up or down based on demand. IaC enables dynamic scaling by automatically adjusting the network infrastructure to accommodate changes in traffic or service requirements. Version Control and Rollbacks: Versioning: IaC tools enable version control for network configurations. This means that changes to the network can be tracked over time, and different versions can be managed. Rollback Capabilities: In case of issues or failures, IaC allows for easy rollback to a previous version of the network configuration. This helps in quickly reverting to a known and working state. Collaboration and Documentation: Collaboration: IaC facilitates collaboration among network operations teams by allowing them to work on the same codebase. This promotes better communication and coordination in managing network infrastructure. Documentation: The code itself serves as documentation for the network configuration. This makes it easier for teams to understand how the network is set up and configured, improving knowledge sharing and troubleshooting. Policy Compliance: Enforcement of Policies: IaC can be used to enforce network policies and compliance standards. This ensures that the network configurations adhere to security, regulatory, and operational requirements. Testing and Validation: Automated Testing: IaC allows for the creation of automated tests to validate network configurations before deployment. This helps catch potential issues early in the development process, reducing the likelihood of errors in the live network. Cost Optimization: Resource Optimization: IaC can help optimize resource usage by dynamically adjusting network resources based on demand. This can lead to cost savings by ensuring that resources are allocated efficiently.","title":"Infrastructure as Code"},{"location":"notes/05-git/#gitops-in-telecom","text":"Repository Structure : Create a Git repository to store configuration files for network equipment. This repository becomes the single source of truth for the desired state of the network. Organize the repository with directories for different types of equipment, such as routers, switches, and firewalls. Infrastructure as Code (IaC) : Represent network configurations as code using Infrastructure as Code (IaC) principles. This means defining network configurations in a declarative format that can be version-controlled in Git. Version Control : Leverage Git's version control capabilities to track changes to network configurations over time. Each commit should represent a specific configuration state, making it easy to roll back to previous configurations if needed. Branching Strategy : Implement a branching strategy in Git to manage different environments or stages of the network (e.g., development, testing, production). This allows for parallel development and testing of configurations. Continuous Integration/Continuous Deployment (CI/CD) : Set up CI/CD pipelines to automate the testing and deployment of network configurations. Changes pushed to the Git repository trigger these pipelines, ensuring that configurations are tested before being applied to the actual network equipment. Git Hooks for Automation : Use Git hooks to trigger automation scripts whenever changes are pushed to the repository. These scripts can validate configurations, generate documentation, and initiate deployment processes. Secrets Management : Implement secure handling of sensitive information, such as device credentials and access keys. Use tools or practices for managing secrets securely within the GitOps workflow. Reconciliation : Employ reconciliation mechanisms to continuously compare the actual state of network equipment with the declared state in the Git repository. Automated tools can be used to detect and correct any discrepancies. Monitoring and Auditing : Integrate monitoring tools to keep track of the network's operational status. Implement logging and auditing mechanisms to trace changes and activities related to network configurations. Collaboration and Code Review : Facilitate collaboration among network engineers by enabling them to review and comment on proposed changes through Git pull requests. This helps ensure that configurations are thoroughly reviewed before being merged. Documentation : Use the Git repository not only for configurations but also for storing documentation related to network equipment. This documentation can include network diagrams, change logs, and other relevant information.","title":"GitOps in Telecom"},{"location":"notes/05-git/#working-with-git","text":"Configure Git: git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\" Initialize a Git Repository git init Clone a Repository: git clone <repository_url> Creates a copy of a remote repository on your local machine.","title":"Working with Git"},{"location":"notes/05-git/#working-with-changes","text":"Check Status: git status Shows the status of changes as untracked, modified, or staged. Stage Changes: git add <file(s)> Adds changes to the staging area in preparation for a commit. Commit Changes: git commit -m \"Commit message\" Records staged changes with a descriptive commit message.","title":"Working with Changes"},{"location":"notes/05-git/#branching-and-merging","text":"Create a Branch: git branch <branch_name> Creates a new branch. Switch Branch: git checkout <branch_name> Switches to the specified branch. Merge Branch: `bash git merge <branch_name> Combines changes from the specified branch into the current branch.","title":"Branching and Merging"},{"location":"notes/05-git/#working-with-remotes","text":"Add a Remote Repository: git remote add <remote_name> <remote_url> Adds a remote repository. Push Changes to a Remote Repository: git push <remote_name> <branch_name> Pushes committed changes to a remote repository. Pull Changes from a Remote Repository: git pull <remote_name> <branch_name> Fetches and merges changes from a remote repository.","title":"Working with Remotes"},{"location":"notes/05-git/#checking-history","text":"View Commit History: git log Displays a log of commits. Show Changes in a Commit: git show <commit_hash> Displays the changes introduced by a specific commit. Downloading and installing Git : If you don't already have Git installed, you can download Git at www.git-scm.com. If you need additional assistance installing Git, you can find more information in the ProGit chapter on installing Git. Now is a good time to create a shortcut to the command-line application you will want to use with Git: If you are working on Windows, It is highly recommended to use Git Bash which is installed with the Git package, so that you can follow along with the facilitator who will be using Bash.If you are working on macOS or another Unix-like system, you can use the built-in Terminal application Clone the following repository Open your chosen shell, and type: git clone https://github.com/mohitkr05/TelcoBootcamp.git If the clone is successful you'll see: $ git clone https://github.com/mohitkr05/TelcoBootcamp.git Cloning into 'TelcoBootcamp'... remote: Counting objects: 6, done. remote: Compressing objects: 100% (2/2), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (6/6), done. Assignment Excercise - Git Read the Pro Git Ebook Watch the Linux command line video","title":"Checking History"},{"location":"notes/06-virtualization/","text":"Virtualization Virtualization is the creation of substitutes for real resources. Understanding Virtualization Definition A system or approach for partitioning the resources of computer hardware into multiple execution environments, which is achieved through the application of various concepts or technologies. This includes but is not limited to hardware and software partitioning, time-sharing, partial or complete machine simulation, emulation, quality of service, and numerous other methodologies. Virtualization Virtualization is a technology that enables the creation and management of virtual instances of computing resources, such as servers, storage, and networks. It allows multiple operating systems (OS) and applications to run on a single physical machine, known as the host, thereby optimizing resource utilization, improving efficiency, and providing flexibility in managing IT infrastructure. Here are some key notes on virtualization: Types of Virtualization Server Virtualization: Involves dividing a physical server into multiple virtual servers, each running its own OS. Desktop Virtualization: Enables running multiple desktop instances on a single physical machine or in a centralized server. Network Virtualization: Abstracts network resources, allowing the creation of virtual networks that operate independently of the underlying physical network. Benefits Consolidation & Resource Optimization : Efficiently utilize hardware resources by running multiple virtual instances on a single physical server. Increase server utilization Simplify legacy software migration Host mixed operating systems per physical platform Streamline test and development environments Isolation : Isolate applications and operating systems, preventing issues in one virtual machine from affecting others. Isolate software faults Reallocate existing partitions Create dedicated or as-needed failover partitions Cost Savings : Reduce hardware and maintenance costs by consolidating multiple servers onto a single physical machine. Flexibility : Easily scale up or down, allocate resources dynamically, and migrate virtual machines between hosts. Hypervisor Also known as a Virtual Machine Monitor (VMM), it is a software or firmware that creates and runs virtual machines on the host system. Two types: Type 1 (bare-metal) runs directly on the hardware, while Type 2 (hosted) runs on top of an existing operating system. Virtual Machine (VM) A software-based emulation of a physical computer, running an operating system and applications independently of the host system. VMs share the underlying physical resources but operate in isolated environments. Virtual Machine Terminology Snapshot A point-in-time image of a virtual machine's state, allowing for easy backup, recovery, and rollback to a specific configuration. Containerization A lightweight form of virtualization that encapsulates applications and their dependencies, allowing them to run consistently across different environments. Cloud Computing and Virtualization Virtualization is a fundamental technology in cloud computing, enabling the efficient allocation of resources and rapid deployment of services. Challenges Resource Overhead: Virtualization introduces some overhead due to the need for the hypervisor to manage virtual instances. Security Concerns: Isolating virtual machines is critical to prevent security breaches and unauthorized access. Complexity: Managing a virtualized environment can be complex, requiring skills in both virtualization technologies and underlying systems. NFV and NFVi NFVi Openstack DPDK DPDK","title":"Virtualization"},{"location":"notes/06-virtualization/#virtualization","text":"Virtualization is the creation of substitutes for real resources.","title":"Virtualization"},{"location":"notes/06-virtualization/#understanding-virtualization","text":"","title":"Understanding Virtualization"},{"location":"notes/06-virtualization/#definition","text":"A system or approach for partitioning the resources of computer hardware into multiple execution environments, which is achieved through the application of various concepts or technologies. This includes but is not limited to hardware and software partitioning, time-sharing, partial or complete machine simulation, emulation, quality of service, and numerous other methodologies.","title":"Definition"},{"location":"notes/06-virtualization/#virtualization_1","text":"Virtualization is a technology that enables the creation and management of virtual instances of computing resources, such as servers, storage, and networks. It allows multiple operating systems (OS) and applications to run on a single physical machine, known as the host, thereby optimizing resource utilization, improving efficiency, and providing flexibility in managing IT infrastructure. Here are some key notes on virtualization:","title":"Virtualization"},{"location":"notes/06-virtualization/#types-of-virtualization","text":"Server Virtualization: Involves dividing a physical server into multiple virtual servers, each running its own OS. Desktop Virtualization: Enables running multiple desktop instances on a single physical machine or in a centralized server. Network Virtualization: Abstracts network resources, allowing the creation of virtual networks that operate independently of the underlying physical network.","title":"Types of Virtualization"},{"location":"notes/06-virtualization/#benefits","text":"Consolidation & Resource Optimization : Efficiently utilize hardware resources by running multiple virtual instances on a single physical server. Increase server utilization Simplify legacy software migration Host mixed operating systems per physical platform Streamline test and development environments Isolation : Isolate applications and operating systems, preventing issues in one virtual machine from affecting others. Isolate software faults Reallocate existing partitions Create dedicated or as-needed failover partitions Cost Savings : Reduce hardware and maintenance costs by consolidating multiple servers onto a single physical machine. Flexibility : Easily scale up or down, allocate resources dynamically, and migrate virtual machines between hosts.","title":"Benefits"},{"location":"notes/06-virtualization/#hypervisor","text":"Also known as a Virtual Machine Monitor (VMM), it is a software or firmware that creates and runs virtual machines on the host system. Two types: Type 1 (bare-metal) runs directly on the hardware, while Type 2 (hosted) runs on top of an existing operating system.","title":"Hypervisor"},{"location":"notes/06-virtualization/#virtual-machine-vm","text":"A software-based emulation of a physical computer, running an operating system and applications independently of the host system. VMs share the underlying physical resources but operate in isolated environments.","title":"Virtual Machine (VM)"},{"location":"notes/06-virtualization/#virtual-machine-terminology","text":"","title":"Virtual Machine Terminology"},{"location":"notes/06-virtualization/#snapshot","text":"A point-in-time image of a virtual machine's state, allowing for easy backup, recovery, and rollback to a specific configuration.","title":"Snapshot"},{"location":"notes/06-virtualization/#containerization","text":"A lightweight form of virtualization that encapsulates applications and their dependencies, allowing them to run consistently across different environments.","title":"Containerization"},{"location":"notes/06-virtualization/#cloud-computing-and-virtualization","text":"Virtualization is a fundamental technology in cloud computing, enabling the efficient allocation of resources and rapid deployment of services.","title":"Cloud Computing and Virtualization"},{"location":"notes/06-virtualization/#challenges","text":"Resource Overhead: Virtualization introduces some overhead due to the need for the hypervisor to manage virtual instances. Security Concerns: Isolating virtual machines is critical to prevent security breaches and unauthorized access. Complexity: Managing a virtualized environment can be complex, requiring skills in both virtualization technologies and underlying systems.","title":"Challenges"},{"location":"notes/06-virtualization/#nfv-and-nfvi","text":"NFVi","title":"NFV and NFVi"},{"location":"notes/06-virtualization/#openstack","text":"","title":"Openstack"},{"location":"notes/06-virtualization/#dpdk","text":"DPDK","title":"DPDK"},{"location":"notes/DPDK/","text":"DPDK & SR-IOV What is DPDK? Data Plane Development Kit (DPDK) is a set of libraries and drivers that facilitate the development of high-performance packet processing applications. DPDK (Data Plane Development Kit) was developed by Intel to accelerate packet processing and was released to the public in the year 2010. It accelerates network I/O by allowing data packets to transition to the user space directly without having to go through kernel space, thus avoiding overheads such as memory copying and context switching. What is SR-IOV? SR-IOV, an acronym for Single Root I/O Virtualization, serves the purpose of partitioning a singular physical function, such as a Network Interface Card (NIC) , into multiple virtual functions . These virtual functions act as independent network interfaces accessible through application The concept behind SR-IOV involves creating virtual functions that mimic direct access to the NIC, offering accelerated Input/Output (IO) by bypassing both the Virtual Machine Manager and the Virtual Switch SR-IOV exhibits speed advantages by circumventing certain NIC functions. It demonstrates efficient scalability without excessive resource consumption, making it a preferred choice for high-performance virtual computing environments. Packet processing Packet processing in Linux involves the handling and manipulation of network packets as they traverse the networking stack. The Linux kernel is responsible for managing the reception, transmission, and routing of packets. The Linux networking stack is a layered architecture that handles various networking tasks. - Key layers include the Data Link Layer, Network Layer, Transport Layer, and Application Layer. Traditional Kernel-Based Stack Packets traverse a multi-layered stack within the kernel, involving tasks like buffering, demultiplexing, protocol processing, and socket-based delivery to applications. There are two primary methods for Network Interface Card (NIC) communication with the kernel: interrupts and polling. Both techniques can be employed independently, or a combination of both may be utilized. Interrupt-Driven I/O - Currently, interrupt-driven I/O is the more prevalent technique. - Commonly used for its efficiency in handling I/O operations, particularly in standard scenarios. Polling for High-Throughput I/O Polling is more frequently applied in situations demanding high-throughput I/O. Device drivers may be configured to switch to polling when the I/O rate is high, reverting to interrupts when the I/O rate decreases. Flexible Configuration The choice between interrupts and polling, or a combination, provides flexibility based on specific I/O requirements. Allows optimization for varying scenarios, adjusting to the characteristics of the workload or I/O rate. SK_BUFF SK_BUFF (Socket Buffer) is a fundamental data structure in the Linux kernel that represents network packets. It plays a crucial role in packet processing, buffering, and transmission across various network layers. Key characteristics - Packet Metadata and Data: - Stores packet data itself, along with essential metadata: - Length - Checksum status - Protocol type - Timestamps - Network device information - Routing information - Quality of Service (QoS) tags - More - Doubly Linked List Organization: - SK_BUFFs form doubly linked lists, enabling efficient movement between queues and processing modules within the kernel. - Buffer Management: - Manages packet data buffers flexibly, using techniques like: - Linear buffers for small packets - Fragmented buffers for large packets - Zero-copy mechanisms to minimize data movement - Function Pointers: - Includes function pointers for custom packet processing actions, allowing for protocol-specific handling and flexibility. - Key uses within the kernel: - Network Device Drivers receive and transmit packets using SK_BUFFs. - Network Stack Layers Process and forward packets through layers like IP, TCP, and UDP. - Routing - Determine packet paths and forward them accordingly. - Traffic Control - Implement QoS features and traffic shaping. - Firewalls and Security Modules - Inspect and filter packets for security purposes. Differences between Packet processing techniques Feature Traditional Kernel-Based Processing Kernel Bypass Processing Execution Location In the kernel space, involving the operating system's networking stack In user space, bypassing the kernel for direct hardware access Packet Reception Interrupt-driven, involving kernel handling and context switches Polling or event-driven, reducing reliance on interrupts Packet Transmission Relying on kernel for transmission, involving context switches Direct user-space control over NICs, minimal kernel involvement Memory Access Utilizes kernel-managed memory, may involve additional overhead Manages its own memory pools in user space for optimized access Latency Generally higher latency due to kernel involvement Lower latency, especially in scenarios requiring rapid packet processing Scalability Limited scalability, particularly in high-throughput scenarios Improved scalability, leveraging user space and parallel processing Context Switching Frequent context switches between user and kernel space Reduced context switching, enhancing efficiency and performance Ease of Customization Limited customization due to reliance on kernel networking stack More flexibility for customization and optimization in user space Compatibility with Hardware Adaptable to various hardware configurations May require hardware-specific optimizations for full efficiency Use Cases General-purpose networking tasks, suitable for standard applications High-performance scenarios like high-frequency trading, packet processing applications Examples of Technologies TCP/IP stack in the kernel space Data Plane Development Kit (DPDK), Netmap, DPACC (Direct Packet Access) DPDK Within DPDK, there are Poll Mode Drivers (PMDs) for 1, 10, and 40 Gigabit, along with other options. PMDs serve as software components with APIs, enabling interaction with Network Interface Cards (NICs) and their queues directly from user space. By utilizing PMDs, DPDK applications gain the capability to access RX and TX descriptors without relying on interrupts, eliminating the necessity for the kernel to manage network traffic. Components of DPDK Environment Abstraction Layer (EAL): The EAL provides a uniform and consistent interface for DPDK applications to interact with the underlying hardware and operating system. It abstracts the differences between various environments, allowing DPDK to run seamlessly across different platforms. Memory Pool Manager (rte_mempool): DPDK's MPM is responsible for managing memory pools efficiently. It allows applications to allocate and deallocate memory in a way that minimizes fragmentation and optimizes performance. Memory pools are crucial for DPDK applications to efficiently manage memory for packet buffers. Poll-Mode Drivers (PMDs): PMDs are essential components that enable DPDK applications to communicate directly with Network Interface Cards (NICs) in user space. PMDs provide APIs for interacting with the NICs and their queues without relying on the kernel for processing interrupts. They play a crucial role in achieving high-throughput and low-latency packet processing. Network Packet Buffer Management (rte_mbuf): DPDK includes a buffer management system that efficiently handles the allocation and management of packet buffers. It ensures that the application has fast and direct access to buffers for packet processing, optimizing memory usage and minimizing packet processing overhead. Ring Manager(rte_ring): The Ring Library is a part of DPDK that provides a lockless, multi-producer, multi-consumer ring buffer implementation. It is used for inter-core communication and synchronization in a multi-core environment. Rings are employed for passing packets and metadata between different cores without the need for locks. Packet Framework (Pktmbuf): DPDK's Packet Framework, also known as Pktmbuf, is a packet buffer management library. It provides an abstraction for packet buffers, making it easier for applications to work with and process packets. Pktmbuf includes features for efficient packet manipulation and metadata storage. Other components & features Multi-Core Support: DPDK is designed to take advantage of multi-core architectures. It provides mechanisms for distributing packet processing tasks across multiple cores, allowing for parallelism and scalability. Multi-core support is crucial for achieving high-performance packet processing in modern, multi-core systems. Device-agnostic API: DPDK provides a device-agnostic API that abstracts the differences between various NICs. This allows DPDK applications to be portable across different hardware platforms without requiring significant modifications. Poll-Mode Event Notification Framework: The Poll-Mode Event Notification Framework in DPDK facilitates event-driven packet processing. It allows applications to receive notifications about specific events, such as the arrival of packets or changes in the status of queues, without relying on interrupts. This framework contributes to the efficiency and responsiveness of DPDK applications. Types of Packet processing DPDK applications for packet processing can be categorized into two different types: run-to-completion or pipeline model Run-to-Completion mode In the Run-to-Completion mode, the entire packet processing or network-related tasks are handled by a single thread within the application. This thread is responsible for executing the tasks in a sequential order. - Incoming packets are typically placed in a queue. The single-threaded application dequeues tasks from this queue and processes them one at a time. - The thread processes each task sequentially, meaning that it completes one task before moving on to the next. This sequential execution ensures determinism in the order of task processing. - Since all tasks are processed by a single thread, there is minimal context switching overhead. Context switching refers to the process of saving and restoring the state of a CPU, which can be resource-intensive. Pipeline Model This model involves breaking down a complex operation or set of tasks into smaller, independent stages, each handled by a dedicated processing unit or thread. - The overall operation or task is divided into several stages, each representing a distinct step in the processing pipeline. These stages can include tasks like packet parsing, filtering, transformation, and transmission. - Different stages of the pipeline operate concurrently, allowing for parallel processing of multiple tasks. This parallelism improves throughput and reduces the overall processing time. - After completing a stage, a processing unit hands over the partially processed data or task to the next stage in the pipeline. This handover is often achieved through shared data structures or buffers. - By utilizing parallelism, the pipeline model ensures more efficient use of available resources, including CPU cores. It maximizes the throughput by allowing multiple tasks to progress through the pipeline simultaneously. - Communication between stages is often required for tasks like passing intermediate results or coordinating decisions. Shared data structures or inter-thread communication mechanisms facilitate this coordination. Differences Feature Run-to-Completion Model Pipeline Model Execution Model Single-threaded, run-to-completion Multi-threaded, pipeline processing Thread Utilization Uses a single thread per core Uses multiple threads per core, each dedicated to a specific stage in the pipeline Task Execution Order Sequential execution of tasks Parallel execution of tasks in stages Concurrency Limited concurrency Higher concurrency, better parallelism Scalability Limited scalability with increasing core count Improved scalability with better utilization of multi-core CPUs Synchronization Minimal synchronization requirements Requires synchronization between stages Complexity Simplicity in design and debugging More complex due to inter-stage communication and synchronization Latency Lower latency for individual tasks Potentially higher latency due to inter-stage communication Throughput May limit overall system throughput Potential for higher throughput, especially in scenarios with parallelizable tasks Use Cases Simple applications with limited parallelism Complex packet processing applications with multiple stages of processing Example Application Simple packet forwarding Deep packet inspection, network functions virtualization (NFV) SR-IOV & DPDK SR-IOV (Single Root I/O Virtualization) and DPDK (Data Plane Development Kit) are often used together in implementing 5G network functions to achieve optimal performance and efficiency. Here are the key reasons for their joint utilization in 5G networks: High Performance and Throughput: SR-IOV allows the creation of multiple virtual functions for a single physical NIC, enabling direct and efficient communication between the virtual functions and the hardware. This reduces the overhead associated with traditional networking, leading to higher performance. DPDK provides a set of libraries and drivers optimized for fast packet processing in user space. It enables applications to interact directly with NICs using Poll-Mode Drivers (PMDs), thereby achieving high throughput and low latency. Efficient Resource Utilization: SR-IOV enables the creation of virtual NICs with dedicated resources, ensuring efficient utilization of hardware capabilities. Each virtual function has direct access to the NIC, avoiding unnecessary layers in the networking stack. DPDK's user-space packet processing minimizes resource overhead associated with kernel-based processing, allowing for efficient utilization of CPU cores and memory. Scalability: 5G networks demand scalability to handle a massive number of devices and the associated increase in network traffic. SR-IOV's ability to create multiple virtual functions and DPDK's support for multi-core architectures provide scalability to meet the demands of 5G networks. Low Latency: DPDK's direct access to NICs in user space and SR-IOV's reduction of overhead contribute to lower latency in packet processing. This is crucial in 5G networks, especially for applications that require real-time communication, such as ultra-reliable low-latency communication (URLLC). Network Function Virtualization (NFV): NFV is a key aspect of 5G networks, allowing network functions to be virtualized and run on standard hardware. SR-IOV and DPDK play significant roles in NFV by providing efficient ways to handle I/O operations and accelerate packet processing. Customization and Optimization: DPDK allows developers to have fine-grained control over packet processing, enabling customization and optimization based on the specific requirements of 5G network functions. SR-IOV's direct access to physical resources allows for customization of virtual functions, tailoring them to the needs of specific network functions in the 5G environment. Complementary Technologies: SR-IOV and DPDK are complementary technologies; SR-IOV optimizes I/O operations and resource allocation, while DPDK optimizes the data plane for packet processing. Their combined use addresses different aspects of networking, resulting in a comprehensive solution for 5G network functions.","title":"DPDK & SR-IOV"},{"location":"notes/DPDK/#dpdk-sr-iov","text":"","title":"DPDK &amp; SR-IOV"},{"location":"notes/DPDK/#what-is-dpdk","text":"Data Plane Development Kit (DPDK) is a set of libraries and drivers that facilitate the development of high-performance packet processing applications. DPDK (Data Plane Development Kit) was developed by Intel to accelerate packet processing and was released to the public in the year 2010. It accelerates network I/O by allowing data packets to transition to the user space directly without having to go through kernel space, thus avoiding overheads such as memory copying and context switching.","title":"What is DPDK?"},{"location":"notes/DPDK/#what-is-sr-iov","text":"SR-IOV, an acronym for Single Root I/O Virtualization, serves the purpose of partitioning a singular physical function, such as a Network Interface Card (NIC) , into multiple virtual functions . These virtual functions act as independent network interfaces accessible through application The concept behind SR-IOV involves creating virtual functions that mimic direct access to the NIC, offering accelerated Input/Output (IO) by bypassing both the Virtual Machine Manager and the Virtual Switch SR-IOV exhibits speed advantages by circumventing certain NIC functions. It demonstrates efficient scalability without excessive resource consumption, making it a preferred choice for high-performance virtual computing environments.","title":"What is SR-IOV?"},{"location":"notes/DPDK/#packet-processing","text":"Packet processing in Linux involves the handling and manipulation of network packets as they traverse the networking stack. The Linux kernel is responsible for managing the reception, transmission, and routing of packets. The Linux networking stack is a layered architecture that handles various networking tasks. - Key layers include the Data Link Layer, Network Layer, Transport Layer, and Application Layer.","title":"Packet processing"},{"location":"notes/DPDK/#traditional-kernel-based-stack","text":"Packets traverse a multi-layered stack within the kernel, involving tasks like buffering, demultiplexing, protocol processing, and socket-based delivery to applications. There are two primary methods for Network Interface Card (NIC) communication with the kernel: interrupts and polling. Both techniques can be employed independently, or a combination of both may be utilized. Interrupt-Driven I/O - Currently, interrupt-driven I/O is the more prevalent technique. - Commonly used for its efficiency in handling I/O operations, particularly in standard scenarios. Polling for High-Throughput I/O Polling is more frequently applied in situations demanding high-throughput I/O. Device drivers may be configured to switch to polling when the I/O rate is high, reverting to interrupts when the I/O rate decreases. Flexible Configuration The choice between interrupts and polling, or a combination, provides flexibility based on specific I/O requirements. Allows optimization for varying scenarios, adjusting to the characteristics of the workload or I/O rate.","title":"Traditional Kernel-Based Stack"},{"location":"notes/DPDK/#sk_buff","text":"SK_BUFF (Socket Buffer) is a fundamental data structure in the Linux kernel that represents network packets. It plays a crucial role in packet processing, buffering, and transmission across various network layers. Key characteristics - Packet Metadata and Data: - Stores packet data itself, along with essential metadata: - Length - Checksum status - Protocol type - Timestamps - Network device information - Routing information - Quality of Service (QoS) tags - More - Doubly Linked List Organization: - SK_BUFFs form doubly linked lists, enabling efficient movement between queues and processing modules within the kernel. - Buffer Management: - Manages packet data buffers flexibly, using techniques like: - Linear buffers for small packets - Fragmented buffers for large packets - Zero-copy mechanisms to minimize data movement - Function Pointers: - Includes function pointers for custom packet processing actions, allowing for protocol-specific handling and flexibility. - Key uses within the kernel: - Network Device Drivers receive and transmit packets using SK_BUFFs. - Network Stack Layers Process and forward packets through layers like IP, TCP, and UDP. - Routing - Determine packet paths and forward them accordingly. - Traffic Control - Implement QoS features and traffic shaping. - Firewalls and Security Modules - Inspect and filter packets for security purposes.","title":"SK_BUFF"},{"location":"notes/DPDK/#differences-between-packet-processing-techniques","text":"Feature Traditional Kernel-Based Processing Kernel Bypass Processing Execution Location In the kernel space, involving the operating system's networking stack In user space, bypassing the kernel for direct hardware access Packet Reception Interrupt-driven, involving kernel handling and context switches Polling or event-driven, reducing reliance on interrupts Packet Transmission Relying on kernel for transmission, involving context switches Direct user-space control over NICs, minimal kernel involvement Memory Access Utilizes kernel-managed memory, may involve additional overhead Manages its own memory pools in user space for optimized access Latency Generally higher latency due to kernel involvement Lower latency, especially in scenarios requiring rapid packet processing Scalability Limited scalability, particularly in high-throughput scenarios Improved scalability, leveraging user space and parallel processing Context Switching Frequent context switches between user and kernel space Reduced context switching, enhancing efficiency and performance Ease of Customization Limited customization due to reliance on kernel networking stack More flexibility for customization and optimization in user space Compatibility with Hardware Adaptable to various hardware configurations May require hardware-specific optimizations for full efficiency Use Cases General-purpose networking tasks, suitable for standard applications High-performance scenarios like high-frequency trading, packet processing applications Examples of Technologies TCP/IP stack in the kernel space Data Plane Development Kit (DPDK), Netmap, DPACC (Direct Packet Access)","title":"Differences between Packet processing techniques"},{"location":"notes/DPDK/#dpdk","text":"Within DPDK, there are Poll Mode Drivers (PMDs) for 1, 10, and 40 Gigabit, along with other options. PMDs serve as software components with APIs, enabling interaction with Network Interface Cards (NICs) and their queues directly from user space. By utilizing PMDs, DPDK applications gain the capability to access RX and TX descriptors without relying on interrupts, eliminating the necessity for the kernel to manage network traffic.","title":"DPDK"},{"location":"notes/DPDK/#components-of-dpdk","text":"Environment Abstraction Layer (EAL): The EAL provides a uniform and consistent interface for DPDK applications to interact with the underlying hardware and operating system. It abstracts the differences between various environments, allowing DPDK to run seamlessly across different platforms. Memory Pool Manager (rte_mempool): DPDK's MPM is responsible for managing memory pools efficiently. It allows applications to allocate and deallocate memory in a way that minimizes fragmentation and optimizes performance. Memory pools are crucial for DPDK applications to efficiently manage memory for packet buffers. Poll-Mode Drivers (PMDs): PMDs are essential components that enable DPDK applications to communicate directly with Network Interface Cards (NICs) in user space. PMDs provide APIs for interacting with the NICs and their queues without relying on the kernel for processing interrupts. They play a crucial role in achieving high-throughput and low-latency packet processing. Network Packet Buffer Management (rte_mbuf): DPDK includes a buffer management system that efficiently handles the allocation and management of packet buffers. It ensures that the application has fast and direct access to buffers for packet processing, optimizing memory usage and minimizing packet processing overhead. Ring Manager(rte_ring): The Ring Library is a part of DPDK that provides a lockless, multi-producer, multi-consumer ring buffer implementation. It is used for inter-core communication and synchronization in a multi-core environment. Rings are employed for passing packets and metadata between different cores without the need for locks. Packet Framework (Pktmbuf): DPDK's Packet Framework, also known as Pktmbuf, is a packet buffer management library. It provides an abstraction for packet buffers, making it easier for applications to work with and process packets. Pktmbuf includes features for efficient packet manipulation and metadata storage.","title":"Components of DPDK"},{"location":"notes/DPDK/#other-components-features","text":"Multi-Core Support: DPDK is designed to take advantage of multi-core architectures. It provides mechanisms for distributing packet processing tasks across multiple cores, allowing for parallelism and scalability. Multi-core support is crucial for achieving high-performance packet processing in modern, multi-core systems. Device-agnostic API: DPDK provides a device-agnostic API that abstracts the differences between various NICs. This allows DPDK applications to be portable across different hardware platforms without requiring significant modifications. Poll-Mode Event Notification Framework: The Poll-Mode Event Notification Framework in DPDK facilitates event-driven packet processing. It allows applications to receive notifications about specific events, such as the arrival of packets or changes in the status of queues, without relying on interrupts. This framework contributes to the efficiency and responsiveness of DPDK applications.","title":"Other components &amp; features"},{"location":"notes/DPDK/#types-of-packet-processing","text":"DPDK applications for packet processing can be categorized into two different types: run-to-completion or pipeline model","title":"Types of Packet processing"},{"location":"notes/DPDK/#run-to-completion-mode","text":"In the Run-to-Completion mode, the entire packet processing or network-related tasks are handled by a single thread within the application. This thread is responsible for executing the tasks in a sequential order. - Incoming packets are typically placed in a queue. The single-threaded application dequeues tasks from this queue and processes them one at a time. - The thread processes each task sequentially, meaning that it completes one task before moving on to the next. This sequential execution ensures determinism in the order of task processing. - Since all tasks are processed by a single thread, there is minimal context switching overhead. Context switching refers to the process of saving and restoring the state of a CPU, which can be resource-intensive.","title":"Run-to-Completion mode"},{"location":"notes/DPDK/#pipeline-model","text":"This model involves breaking down a complex operation or set of tasks into smaller, independent stages, each handled by a dedicated processing unit or thread. - The overall operation or task is divided into several stages, each representing a distinct step in the processing pipeline. These stages can include tasks like packet parsing, filtering, transformation, and transmission. - Different stages of the pipeline operate concurrently, allowing for parallel processing of multiple tasks. This parallelism improves throughput and reduces the overall processing time. - After completing a stage, a processing unit hands over the partially processed data or task to the next stage in the pipeline. This handover is often achieved through shared data structures or buffers. - By utilizing parallelism, the pipeline model ensures more efficient use of available resources, including CPU cores. It maximizes the throughput by allowing multiple tasks to progress through the pipeline simultaneously. - Communication between stages is often required for tasks like passing intermediate results or coordinating decisions. Shared data structures or inter-thread communication mechanisms facilitate this coordination.","title":"Pipeline Model"},{"location":"notes/DPDK/#differences","text":"Feature Run-to-Completion Model Pipeline Model Execution Model Single-threaded, run-to-completion Multi-threaded, pipeline processing Thread Utilization Uses a single thread per core Uses multiple threads per core, each dedicated to a specific stage in the pipeline Task Execution Order Sequential execution of tasks Parallel execution of tasks in stages Concurrency Limited concurrency Higher concurrency, better parallelism Scalability Limited scalability with increasing core count Improved scalability with better utilization of multi-core CPUs Synchronization Minimal synchronization requirements Requires synchronization between stages Complexity Simplicity in design and debugging More complex due to inter-stage communication and synchronization Latency Lower latency for individual tasks Potentially higher latency due to inter-stage communication Throughput May limit overall system throughput Potential for higher throughput, especially in scenarios with parallelizable tasks Use Cases Simple applications with limited parallelism Complex packet processing applications with multiple stages of processing Example Application Simple packet forwarding Deep packet inspection, network functions virtualization (NFV)","title":"Differences"},{"location":"notes/DPDK/#sr-iov-dpdk","text":"SR-IOV (Single Root I/O Virtualization) and DPDK (Data Plane Development Kit) are often used together in implementing 5G network functions to achieve optimal performance and efficiency. Here are the key reasons for their joint utilization in 5G networks: High Performance and Throughput: SR-IOV allows the creation of multiple virtual functions for a single physical NIC, enabling direct and efficient communication between the virtual functions and the hardware. This reduces the overhead associated with traditional networking, leading to higher performance. DPDK provides a set of libraries and drivers optimized for fast packet processing in user space. It enables applications to interact directly with NICs using Poll-Mode Drivers (PMDs), thereby achieving high throughput and low latency. Efficient Resource Utilization: SR-IOV enables the creation of virtual NICs with dedicated resources, ensuring efficient utilization of hardware capabilities. Each virtual function has direct access to the NIC, avoiding unnecessary layers in the networking stack. DPDK's user-space packet processing minimizes resource overhead associated with kernel-based processing, allowing for efficient utilization of CPU cores and memory. Scalability: 5G networks demand scalability to handle a massive number of devices and the associated increase in network traffic. SR-IOV's ability to create multiple virtual functions and DPDK's support for multi-core architectures provide scalability to meet the demands of 5G networks. Low Latency: DPDK's direct access to NICs in user space and SR-IOV's reduction of overhead contribute to lower latency in packet processing. This is crucial in 5G networks, especially for applications that require real-time communication, such as ultra-reliable low-latency communication (URLLC). Network Function Virtualization (NFV): NFV is a key aspect of 5G networks, allowing network functions to be virtualized and run on standard hardware. SR-IOV and DPDK play significant roles in NFV by providing efficient ways to handle I/O operations and accelerate packet processing. Customization and Optimization: DPDK allows developers to have fine-grained control over packet processing, enabling customization and optimization based on the specific requirements of 5G network functions. SR-IOV's direct access to physical resources allows for customization of virtual functions, tailoring them to the needs of specific network functions in the 5G environment. Complementary Technologies: SR-IOV and DPDK are complementary technologies; SR-IOV optimizes I/O operations and resource allocation, while DPDK optimizes the data plane for packet processing. Their combined use addresses different aspects of networking, resulting in a comprehensive solution for 5G network functions.","title":"SR-IOV &amp; DPDK"},{"location":"notes/NFVI/","text":"Network function virtualization Network Functions Virtualization (NFV) is a networking concept that aims to transform traditional network architectures by virtualizing network functions, typically implemented on dedicated hardware appliances. NFV leverages virtualization technologies to move these network functions from proprietary hardware to software-based solutions running on standard servers. By doing so, NFV offers greater flexibility, scalability, and efficiency in deploying and managing network services. Terminology Network virtualization involves abstracting network functions to be controlled and manipulated by software on standardized compute nodes. Network Functions Virtualization (NFV) integrates cloud and virtualization technologies, facilitating the rapid development of scalable and automated network services. NFVi NFV Infrastructure (NFVi) refers to the underlying hardware and software infrastructure that supports the deployment and execution of virtualized network functions. NFVi provides the necessary compute, storage, and networking resources to host virtualized network functions efficiently. It serves as the foundation for the virtualized network services enabled by NFV. NFVi Architecture Virtualization Layer: The virtualization layer consists of hypervisors or container runtimes that enable the creation and management of virtual machines or containers running network functions. Compute Resources: NFVi includes the compute resources, such as servers or virtual machines, where virtualized network functions are instantiated and executed. Storage Resources: Storage components in NFVi provide the necessary storage resources for storing virtual machine images, data, and configurations related to network functions. Networking Resources: NFVi encompasses networking components, including switches, routers, and virtual networks, to facilitate communication between virtualized network functions and external networks. Management and Orchestration (MANO): MANO components within NFVi handle the orchestration, management, and automation of the NFV infrastructure. This includes the Virtualized Infrastructure Manager (VIM) responsible for resource management.","title":"Network function virtualization"},{"location":"notes/NFVI/#network-function-virtualization","text":"Network Functions Virtualization (NFV) is a networking concept that aims to transform traditional network architectures by virtualizing network functions, typically implemented on dedicated hardware appliances. NFV leverages virtualization technologies to move these network functions from proprietary hardware to software-based solutions running on standard servers. By doing so, NFV offers greater flexibility, scalability, and efficiency in deploying and managing network services.","title":"Network function virtualization"},{"location":"notes/NFVI/#terminology","text":"Network virtualization involves abstracting network functions to be controlled and manipulated by software on standardized compute nodes. Network Functions Virtualization (NFV) integrates cloud and virtualization technologies, facilitating the rapid development of scalable and automated network services.","title":"Terminology"},{"location":"notes/NFVI/#nfvi","text":"NFV Infrastructure (NFVi) refers to the underlying hardware and software infrastructure that supports the deployment and execution of virtualized network functions. NFVi provides the necessary compute, storage, and networking resources to host virtualized network functions efficiently. It serves as the foundation for the virtualized network services enabled by NFV.","title":"NFVi"},{"location":"notes/NFVI/#nfvi-architecture","text":"Virtualization Layer: The virtualization layer consists of hypervisors or container runtimes that enable the creation and management of virtual machines or containers running network functions. Compute Resources: NFVi includes the compute resources, such as servers or virtual machines, where virtualized network functions are instantiated and executed. Storage Resources: Storage components in NFVi provide the necessary storage resources for storing virtual machine images, data, and configurations related to network functions. Networking Resources: NFVi encompasses networking components, including switches, routers, and virtual networks, to facilitate communication between virtualized network functions and external networks. Management and Orchestration (MANO): MANO components within NFVi handle the orchestration, management, and automation of the NFV infrastructure. This includes the Virtualized Infrastructure Manager (VIM) responsible for resource management.","title":"NFVi Architecture"},{"location":"notes/cloud-services/","text":"","title":"Cloud services"},{"location":"notes/containers/","text":"","title":"Containers"},{"location":"notes/shellscripting/","text":"Shell Scripting 101 What is shell scripting? What would happen if we write all the linux commands in a file and execute it? Well, shell scripting is the art of automating tasks through the command line. It uses a series of commands in a script file. Writing your first shell script Create your first script: Open your text editor and create a new file. Save it with a descriptive name ending in .sh, like hello_world.sh. Remember, the .sh extension tells the system it's a shell script. Hello, world!: Inside your script, type the following line: echo \"Hello, world!\" This line uses the echo command to print the text \"Hello, world!\" to the terminal. Run your script: Save your script and open a terminal. Navigate to the directory where you saved the script. To run the script, type the following: chmod u+x hello_world.sh bash hello_world.sh sh hello_world.sh ./hello_world.sh If everything went well, you should see \"Hello, world!\" printed in the terminal! Congratulations, you've written and run your first shell script! Using variables Creating a Variable: Think of a variable as a named box holding any value you want: text, numbers, dates, even other variables! To create one, simply assign a value using the = operator: name=\"Bob\" age=3 today=$(date +%Y-%m-%d) # Store today's date Note: - Variable names are case-sensitive! Name and name are different boxes. - No Spaces: Ensure there are no spaces around the = sign. Accessing Stored Values: To use the stored value, add a $ symbol before the variable name: echo \"Hello, $name!\" echo \"I was born in $today.\" Unsetting Variables: Like cleaning up your room, you can remove variables with the unset command: unset name echo \"$name\" # Should print nothing Special variables pecial Variables Shell scripts have several special variables, like: $0 - The name of the script. $1 to $9 - The first to ninth argument passed to the script. $# - The number of arguments passed to the script. $? - The exit status of the last command executed. $$ - The process ID of the current script. $USER - The username of the user running the script. Examples Dates current_date=$(date) echo $current_date Arithmetic a=5 b=3 sum=$((a + b)) echo $sum # Outputs: 8 String greeting=\"Hello, \" name=\"Bob\" message=$greeting$name echo $message # Outputs: Hello, Bob Text printing #!/bin/bash # Declare a variable greeting=\"Hello, Shell Scripting!\" # Use the variable echo $greeting User Input #!/bin/bash # Ask for user input echo \"What is your name?\" read name # Greet the user echo \"Welcome, $name!\" Command line substitution #!/bin/bash # Get the current directory current_directory=$(pwd) # Display the current directory echo \"You are in the directory: $current_directory\" Arithmetic Operations #!/bin/bash # Declare variables a=10 b=5 # Arithmetic operations sum=$((a + b)) difference=$((a - b)) product=$((a * b)) quotient=$((a / b)) # Display the results echo \"Sum: $sum\" echo \"Difference: $difference\" echo \"Product: $product\" echo \"Quotient: $quotient\" Conditional Statements if Statement: It evaluates a condition and, if true, executes a block of code. Here's the basic structure: if condition; then # Code to execute if true fi For example, checking if a file exists: if [ -f \"myfile.txt\" ]; then echo \"myfile.txt is here!\" fi Exploring Comparisons: The condition can be any comparison using operators like ==, !=, <, >, <=, and >=. Let's compare user input to a threshold: read -p \"Enter your age: \" age if [[ $age -ge 18 ]]; then echo \"Welcome! You're eligible to enter.\" else echo \"Sorry, you must be 18 or older.\" fi Else and Elif: Add options with else for the opposite condition and elif for additional checks: read -p \"Enter 1 or 2: \" number if [[ $number -eq 1 ]]; then echo \"Heads!\" elif [[ $number -eq 2 ]]; then echo \"Tails!\" else echo \"It's a draw!\" fi Use code with caution. Learn more Examples Basic If-Else Statement #!/bin/bash # Read a number echo \"Enter a number:\" read number # Check if the number is positive or negative if [ $number -gt 0 ]; then echo \"The number is positive.\" elif [ $number -lt 0 ]; then echo \"The number is negative.\" else echo \"The number is zero.\" fi Read if file exists #!/bin/bash # File name file=\"sample.txt\" # Check if the file exists if [ -f \"$file\" ]; then echo \"$file exists.\" else echo \"$file does not exist.\" fi Compare strings #!/bin/bash # String variables str1=\"Hello\" str2=\"World\" # Compare strings if [ \"$str1\" == \"$str2\" ]; then echo \"Strings are equal.\" else echo \"Strings are not equal.\" fi Using AND (&&) and OR (||) in conditions #!/bin/bash # Variables a=20 # Check conditions if [ $a -gt 5 ] && [ $a -lt 15 ]; then echo \"Number is between 5 and 15.\" elif [ $a -lt 5 ] || [ $a -gt 15 ]; then echo \"Number can be less than 5 or more than 15.\" else echo \"Neither condition is true.\" fi Case statement #!/bin/bash # Ask for user input echo \"Enter your choice (yes/no):\" read choice # Case statement case $choice in yes|YES|y|Y) echo \"You chose yes.\" ;; no|NO|n|N) echo \"You chose no.\" ;; *) echo \"Invalid choice.\" ;; esac File Reading Basic file reading This method uses a while loop to iterate through each line of the file. Use the read command to capture each line within the loop. while IFS= read -r line; do # Process each line (e.g., echo, store in variable) done < \"filename.txt\" Read specific lines head -n 3 \"filename.txt\" # Read first 3 lines tail -n 2 \"filename.txt\" # Read last 2 lines grep \"error\" \"filename.txt\" # Read lines containing \"error\" Read whole file into a variable file_content=$(cat \"filename.txt\") # Process the entire content stored in the variable Example #!/bin/bash # Ask the user for the directory path echo \"Enter the directory path containing text files:\" read directory # Check if the directory exists if [ ! -d \"$directory\" ]; then echo \"Directory does not exist.\" exit 1 fi # Ask for the search keyword echo \"Enter the search keyword:\" read keyword # Loop through each text file in the directory echo \"Searching for '$keyword' in files in $directory:\" for file in \"$directory\"/*.txt do echo \"Searching in $file:\" grep \"$keyword\" \"$file\" done References 1. https://www.shellscript.sh/","title":"Shell Scripting 101"},{"location":"notes/shellscripting/#shell-scripting-101","text":"","title":"Shell Scripting 101"},{"location":"notes/shellscripting/#what-is-shell-scripting","text":"What would happen if we write all the linux commands in a file and execute it? Well, shell scripting is the art of automating tasks through the command line. It uses a series of commands in a script file.","title":"What is shell scripting?"},{"location":"notes/shellscripting/#writing-your-first-shell-script","text":"Create your first script: Open your text editor and create a new file. Save it with a descriptive name ending in .sh, like hello_world.sh. Remember, the .sh extension tells the system it's a shell script. Hello, world!: Inside your script, type the following line: echo \"Hello, world!\" This line uses the echo command to print the text \"Hello, world!\" to the terminal. Run your script: Save your script and open a terminal. Navigate to the directory where you saved the script. To run the script, type the following: chmod u+x hello_world.sh bash hello_world.sh sh hello_world.sh ./hello_world.sh If everything went well, you should see \"Hello, world!\" printed in the terminal! Congratulations, you've written and run your first shell script!","title":"Writing your first shell script"},{"location":"notes/shellscripting/#using-variables","text":"Creating a Variable: Think of a variable as a named box holding any value you want: text, numbers, dates, even other variables! To create one, simply assign a value using the = operator: name=\"Bob\" age=3 today=$(date +%Y-%m-%d) # Store today's date Note: - Variable names are case-sensitive! Name and name are different boxes. - No Spaces: Ensure there are no spaces around the = sign. Accessing Stored Values: To use the stored value, add a $ symbol before the variable name: echo \"Hello, $name!\" echo \"I was born in $today.\" Unsetting Variables: Like cleaning up your room, you can remove variables with the unset command: unset name echo \"$name\" # Should print nothing Special variables pecial Variables Shell scripts have several special variables, like: $0 - The name of the script. $1 to $9 - The first to ninth argument passed to the script. $# - The number of arguments passed to the script. $? - The exit status of the last command executed. $$ - The process ID of the current script. $USER - The username of the user running the script.","title":"Using variables"},{"location":"notes/shellscripting/#examples","text":"Dates current_date=$(date) echo $current_date Arithmetic a=5 b=3 sum=$((a + b)) echo $sum # Outputs: 8 String greeting=\"Hello, \" name=\"Bob\" message=$greeting$name echo $message # Outputs: Hello, Bob Text printing #!/bin/bash # Declare a variable greeting=\"Hello, Shell Scripting!\" # Use the variable echo $greeting User Input #!/bin/bash # Ask for user input echo \"What is your name?\" read name # Greet the user echo \"Welcome, $name!\" Command line substitution #!/bin/bash # Get the current directory current_directory=$(pwd) # Display the current directory echo \"You are in the directory: $current_directory\" Arithmetic Operations #!/bin/bash # Declare variables a=10 b=5 # Arithmetic operations sum=$((a + b)) difference=$((a - b)) product=$((a * b)) quotient=$((a / b)) # Display the results echo \"Sum: $sum\" echo \"Difference: $difference\" echo \"Product: $product\" echo \"Quotient: $quotient\"","title":"Examples"},{"location":"notes/shellscripting/#conditional-statements","text":"if Statement: It evaluates a condition and, if true, executes a block of code. Here's the basic structure: if condition; then # Code to execute if true fi For example, checking if a file exists: if [ -f \"myfile.txt\" ]; then echo \"myfile.txt is here!\" fi Exploring Comparisons: The condition can be any comparison using operators like ==, !=, <, >, <=, and >=. Let's compare user input to a threshold: read -p \"Enter your age: \" age if [[ $age -ge 18 ]]; then echo \"Welcome! You're eligible to enter.\" else echo \"Sorry, you must be 18 or older.\" fi Else and Elif: Add options with else for the opposite condition and elif for additional checks: read -p \"Enter 1 or 2: \" number if [[ $number -eq 1 ]]; then echo \"Heads!\" elif [[ $number -eq 2 ]]; then echo \"Tails!\" else echo \"It's a draw!\" fi Use code with caution. Learn more","title":"Conditional Statements"},{"location":"notes/shellscripting/#examples_1","text":"Basic If-Else Statement #!/bin/bash # Read a number echo \"Enter a number:\" read number # Check if the number is positive or negative if [ $number -gt 0 ]; then echo \"The number is positive.\" elif [ $number -lt 0 ]; then echo \"The number is negative.\" else echo \"The number is zero.\" fi Read if file exists #!/bin/bash # File name file=\"sample.txt\" # Check if the file exists if [ -f \"$file\" ]; then echo \"$file exists.\" else echo \"$file does not exist.\" fi Compare strings #!/bin/bash # String variables str1=\"Hello\" str2=\"World\" # Compare strings if [ \"$str1\" == \"$str2\" ]; then echo \"Strings are equal.\" else echo \"Strings are not equal.\" fi Using AND (&&) and OR (||) in conditions #!/bin/bash # Variables a=20 # Check conditions if [ $a -gt 5 ] && [ $a -lt 15 ]; then echo \"Number is between 5 and 15.\" elif [ $a -lt 5 ] || [ $a -gt 15 ]; then echo \"Number can be less than 5 or more than 15.\" else echo \"Neither condition is true.\" fi Case statement #!/bin/bash # Ask for user input echo \"Enter your choice (yes/no):\" read choice # Case statement case $choice in yes|YES|y|Y) echo \"You chose yes.\" ;; no|NO|n|N) echo \"You chose no.\" ;; *) echo \"Invalid choice.\" ;; esac","title":"Examples"},{"location":"notes/shellscripting/#file-reading","text":"Basic file reading This method uses a while loop to iterate through each line of the file. Use the read command to capture each line within the loop. while IFS= read -r line; do # Process each line (e.g., echo, store in variable) done < \"filename.txt\" Read specific lines head -n 3 \"filename.txt\" # Read first 3 lines tail -n 2 \"filename.txt\" # Read last 2 lines grep \"error\" \"filename.txt\" # Read lines containing \"error\" Read whole file into a variable file_content=$(cat \"filename.txt\") # Process the entire content stored in the variable","title":"File Reading"},{"location":"notes/shellscripting/#example","text":"#!/bin/bash # Ask the user for the directory path echo \"Enter the directory path containing text files:\" read directory # Check if the directory exists if [ ! -d \"$directory\" ]; then echo \"Directory does not exist.\" exit 1 fi # Ask for the search keyword echo \"Enter the search keyword:\" read keyword # Loop through each text file in the directory echo \"Searching for '$keyword' in files in $directory:\" for file in \"$directory\"/*.txt do echo \"Searching in $file:\" grep \"$keyword\" \"$file\" done References 1. https://www.shellscript.sh/","title":"Example"}]}